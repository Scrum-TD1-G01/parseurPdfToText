Preamble : Torres-moreno1998.pdf
Title : Efficient Adaptive Learning for Classification Tasks with Binary Units
Auteur : Communicated by Scott FahlmanEfficient Adaptive Learning for Classification Tasks withBinary UnitsJ. Manuel Torres MorenoMirta B. GordonDépartement de Recherche Fondamentale sur la Matière Condensée, CEA Grenoble,38054 Grenoble Cedex 9, FranceThis article presents a new incremental learning algorithm for classification tasks, called NetLines, which is well adapted for both binaryand real-valued input patterns. It generates small, compact feedforwardneural networks with one hidden layer of binary units and binary outputunits. A convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns.An implementation for problems with more than two classes, validfor any binary classifier, is proposed. The generalization error andthe size of the resulting networks are compared to the best publishedresults on well-known classification benchmarks. Early stopping is shownto decrease overfitting, without improving the generalization performance.
Abstract : 
Biblio : Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Unpublished doctoral dissertation, Ecole Polytechnique Fédérale de Lausanne,Switzerland.Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. PhysicalReview A, 44, 6888.Bottou, L., & Vapnik, V. (1992). Local learning algorithms. Neural Computation,4(6), 888–900.Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Departmentof Statistics, University of California at Berkeley.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classificationand regression trees. Monterey, CA: Wadsworth and Brooks/Cole.Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopfield, J.(1987). Large automatic learning, rule extraction, and generalization. ComplexSystems, 1, 877–922.Depenau, J. (1995). Automated design of neural network architecture for classification.Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.Drucker, H., Schapire, R., & Simard, P. (1993). Improving performance in neural networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42–49). San Mateo, CA: Morgan Kaufmann.Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architecture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,2 (pp. 524–532). San Mateo: Morgan Kaufmann.Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural treenetworks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in NeuralInformation Processing Systems, 6 (pp. 1035–1042). San Mateo, CA: MorganKaufmann.Frean, M. (1990). The Upstart algorithm: A method for constructing and trainingfeedforward neural networks. Neural Computation, 2(2), 198–209.Frean, M. (1992). A “thermal” perceptron learning rule. Neural Computation, 4(6),946–957.Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.Fritzke, B. (1994). Supervised learning with growing cell structures. InJ. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural information processing systems, 6 (pp. 255–262). San Mateo, CA: Morgan Kaufmann.Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. PatternRecognition, Oct. 28–31, Paris, vol. 4.Classification Tasks with Binary Units1029Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).5èmes Journées Nationales du PRC-IA Teknea, Nancy.Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and thebias/variance dilemma. Neural Computation, 4(1), 1–58.Goodman, R. M., Smyth, P., Higgins, C. M., & Miller, J. W. (1992). Rule-basedneural networks for classification and probability estimation. Neural Computation, 4(6), 781–804.Gordon, M. B. (1996). A convergence theorem for incremental learning with realvalued inputs. In IEEE International Conference on Neural Networks, pp. 381–386.Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rulethat finds the optimal weights. In M. Verleysen (Ed.), European Symposium onArtificial Neural Networks (pp. 105–110). Brussels: D Facto.Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperaturedependent algorithm. Europhysics Letters, 29(3), 257–262.Gordon, M. B., Peretto, P., & Berchier, D. (1993). Learning algorithms for perceptrons from statistical physics. Journal of Physics I (France), 3, 377–387.Gorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units in a layerednetwork trained to classify sonar targets. Neural Networks, 1, 75–89.Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. InW. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses. Singapore: World Scientific.Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision usingthe cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:Carnegie Mellon University.Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: Astepwise procedure for building and training a neural network. In J. Hérault& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications(pp. 41–50). Berlin: Springer-Verlag.Marchand, M., Golea, M., & Ruján, P. (1990). A convergence theorem for sequential learning in two-layer perceptrons. Europhysics Letters, 11, 487–492.Martinez, D., & Estève, D. (1992). The offset algorithm: Building and learningmethod for multilayer neural networks. Europhysics Letters, 18, 95–100.Mézard, M., & Nadal, J.-P. (1989). Learning in feedforward layered networks:The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191–2203.Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time algorithm for generating neural networks for pattern classification: Its stabilityproperties and some test results. Neural Computation, 5(2), 317–330.Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural network. Int. J. Neur. Syst., 1, 55–59.Prechelt, L. (1994). PROBEN1—A set of benchmarks and benchmarking rules for neural network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,Faculty of Informatics.Raffin, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror,a temperature dependent learning algorithm. Neural Computation, 7(6), 1206–1224.1030J. Manuel Torres Moreno and Mirta B. GordonReilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for categorylearning. Biological Cybernetics, 45, 35–41.Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithmfor the construction and training of a class of multilayer perceptron. NeuralNetworks, 6(1), 535–545.Sirat, J. A., & Nadal, J.-P. (1990). Neural trees: A new tool for classification.Network, 1, 423–438.Solla, S. A. (1989). Learning and generalization in layered neural networks: Thecontiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networksfrom Models to Applications. Paris: I.D.S.E.T.Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupledwith optimal perceptron learning for classification. In M. Verleysen (Ed.),European Symposium on Artificial Neural Networks. Brussels: D Facto.Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonarsignals benchmark. Neural Proc. Letters, 7(1), 1–4.Trhun, S. B., et al. (1991). The monk’s problems: A performance comparison of differentlearning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: CarnegieMellon University.Vapnik, V. (1992). Principles of risk minimization for learning theory. InJ. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), Advances in neural information processing systems, 4 (pp. 831–838). San Mateo, CA: Morgan Kaufmann.Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neural networks. In M. Verleysen (Ed.), European Symposium on Artificial NeuralNetworks (pp. 359–364). Brussels: D Facto.Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of patternseparation for medical diagnosis applied to breast cytology. In Proceedings ofthe National Academy of Sciences, USA, 87, 9193–9196.Received February 13, 1997; accepted September 4, 1997.This article has been cited by:1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectralanalysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward NeuralNetworks. International Journal of Neural Systems 08, 647-659. [CrossRef]
