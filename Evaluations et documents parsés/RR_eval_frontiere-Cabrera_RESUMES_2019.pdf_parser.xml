b'<opml version="1.0"><article><preamble>Cabrera_RESUMES_2019.pdf</preamble>
<title>b\'Ranking r\\xe9sum\\xe9s automatically using only r\\xe9sum\\xe9s: A method free of job offers\'</title>
<auteur>Expert Systems With Applications 123 (2019) 91&#8211;107 Contents lists available at ScienceDirect Expert Systems With Applications journal homepage: www.elsevier.com/locate/eswa Ranking r&#233;sum&#233;s automatically using only r&#233;sum&#233;s: A method free of job offers Luis Adri&#225;n Cabrera-Diegoa,c,1,&#8727; , Marc El-B&#233;zea , Juan-Manuel Torres-Morenoa,b , Barth&#233;l&#233;my Durettec a LIA, Avignon Universit&#233;, 91022 Chemin des Meinajari&#232;s, Avignon 84022, France b Polytechnique Montr&#233;al, Canada c Adoc Talent Management, 21 Rue du Faubourg Saint-Antoine, Paris 75011, France a r t i c l e i n f o Article history: Received 18 May 2018 Revised 30 November 2018 Accepted 29 December 2018 Available online 31 December 2018 Keywords: R&#233;sum&#233; Curriculum vitae Recommendation system Relevance feedback e-Recruitment Ranking Mean average precision a b s t r a c t With the success of the electronic recruitment, now it is easier to &#64257;nd a job offer and apply for it. How- ever, due to this same success, nowadays, human resource managers tend to receive high volumes of applications for each job offer. These applications turn into large quantities of documents, known as r&#233;- sum&#233;s or curricula vitae, that need to be processed quickly and correctly. To reduce the time necessary to process the r&#233;sum&#233;s, human resource managers have been working with the scienti&#64257;c community to create systems that automate their ranking. Until today, most of these systems are based on the compar- ison of job offers and r&#233;sum&#233;s. Nevertheless, this comparison is impossible to do in data sets where job offers are no longer available, as it happens in this work. We present two methods to rank r&#233;sum&#233;s that do not use job offers or any semantic resource, unlike existing state-of-the-art systems. The methods are based on what we call Inter-R&#233;sum&#233; Proximity, which is the lexical similarity between only r&#233;sum&#233;s sent by candidates in response to the same job offer. Besides, we propose the use of Relevance Feedback, at general and lexical levels to improve the ranking of r&#233;sum&#233;s. Relevance Feedback is applied using tech- niques based on similarity coe&#64259;cients and vocabulary scoring. All the methods have been tested on a large corpus of 171 real selection processes, which correspond to more than 14,000 r&#233;sum&#233;s. The devel- oped methods can rank correctly, in average, 93% of the r&#233;sum&#233;s sent to each job posting. The outcomes presented here show that it is not necessary to use job offers or semantic resources to provide high quality results. Furthermore, we observed that r&#233;sum&#233;s have particular characteristics that as ensemble, work as a facial composite and provide more information about the job posting than the job offer. This certainly will change how systems analyze and rank r&#233;sum&#233;s. &#169; 2019 Elsevier Ltd. All rights reserved. 
</auteur>
<abstract />
<introduction>1. Introduction For at least 15 years, the process of attracting possible candi- dates for a job, i.e., recruitment process, moved from traditional means, like newspapers and job boards, to the Internet and started to be known as electronic recruitment or e-Recruitment (Kessler, B&#233;chet, Roche, Torres-Moreno, &amp; El-B&#232;ze, 2012; Radevski &amp; Trichet, 2006). The success of e-Rectruitment over traditional recruitment pro- cesses lies in the advantages it brings to users and especially to &#8727; Corresponding author. E-mail addresses: diegol@edgehill.ac.uk (L.A. Cabrera-Diego), marc.elbeze@univ- avignon.fr (M. El-B&#233;ze), juan-manuel.torres@univ-avignon.fr (J.-M. Torres-Moreno), durette@adoc-tm.com (B. Durette). 1 Present address: Department of Computing, Edge Hill University, St. Helens Road, L39 4QP Ormskirk, UK Human Resources Managers (HRMs). Today, due to e-Recruitment, job offers can more easily reach not only specialized communi- ties (Arthur, 2001, page 126) but also wider audiences locally, na- tionally or internationally (Montuschi, Gatteschi, Lamberti, Sanna, &amp; Demartini, 2014). HRMs&#8217; operational costs have been reduced, in certain cases to one-twentieth of the original expenses (Chapman &amp; Webster, 2003). Now, job seekers can search for job offers through the Internet (Looser, Ma, &amp; Schewe, 2013) and apply to them faster by sending an e-mail or &#64257;lling out a web form with an electronic r&#233;sum&#233; or CV attached (Elkington, 2005). The great- est e-Recruitment&#8217;s advantage is the possibility of being in con- tact with job seekers, employers and HRM all the time around the world (Barber, 2006, page 1). Although e-Recruitment has helped HRMs with the task of identifying and attracting potential candidates, its use has brought a number of undesirable consequences, especially when high vol- https://doi.org/10.1016/j.eswa.2018.12.054 0957-4174/&#169; 2019 Elsevier Ltd. All rights reserved. 92 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 umes of applications are received (Barber, 2006, page 11). After the recruitment process, an HRM must select the group of applicants that are relevant for the job offered. This selection is performed by manually screening r&#233;sum&#233;s.2 The manual screening consists of examining and comparing applicant information, found in the r&#233;sum&#233;, with respect to the speci&#64257;cations of the position or per- son speci&#64257;cation3 (Armstrong and Taylor, 2014, Page 226). However, given the large number of applications, HRMs have trouble screen- ing them correctly and rapidly (Trichet, Bourse, Lecl&#232;re, &amp; Morin, 2004). Furthermore, HRMs have seen an increase in applications from unquali&#64257;ed candidates (Faliagka, Kozanidis, Stamou, Tsaka- lidis, &amp; Tzimas, 2011), meaning they lose valuable time during the screening process. The scienti&#64257;c community has proposed multiple systems to re- duce the negative impacts of e-Recruitment. The vast majority of the developed systems are based on comparing r&#233;sum&#233;s and job offers, e.g., using measures like Cosine Similarity (Kessler, B&#233;chet, Torres-Moreno, Roche, &amp; El-B&#232;ze, 2009; Singh, Rose, Visweswariah, Chenthamarakshan, &amp; Kambhatla, 2010). In some cases, to im- prove the matching, they include ontologies or semantic resources that are expected to ameliorate the similarity between docu- ments, like those shown in Senthil Kumaran and Sankar (2013) and Montuschi et al. (2014). The work that is here presented occurs in the following con- text. It is the outcome of a collaboration project with a Human Resources enterprise that had a large database of recruitment and selection processes conducted by them previously. The database is divided by job postings4 in which we can &#64257;nd the applications sent by the interested or directly contacted candidates. Each application is composed, at least, of a r&#233;sum&#233; and the outcome of the selec- tion process. This database, however, has a particular characteristic, for most of the job postings, neither the job offer nor the person speci&#64257;cation is available.5 This characteristic is due to the software used to store automatically the incoming applications did not pro- vide the option to keep these documents. Due to the fact that it is impossible to apply state-of-the-art&#8217;s methods for all the database, we decided to explore how to rank r&#233;sum&#233;s without making use of job offers. The result of this ex- ploration are innovative and simple methods that use uniquely the proximity between r&#233;sum&#233;s sent for the same job posting. To this end, we use a similarity measure and Relevance Feedback (Rocchio, 1971) applied with methods based on a similarity quo- tient and a vocabulary scoring. Despite in this work, we do not make use of more complex methods, like deep-learning neural networks, or dense text repre- sentations, i.e., word embedding, the idea of using them was al- ways present. There were several reasons why not to use these 
</introduction><corps>2 According to Thompson (2000), a r&#233;sum&#233;, also known as resume, curriculum vi- tae or CV, is a document prepared by a job candidate, for potential employers, that describes one&#8217;s education, quali&#64257;cations and professional experience. In this paper we will use r&#233;sum&#233; as common term. 3 This is a document detailing which characteristics, mandatory and optional, should be found in a r&#233;sum&#233; according to the employer. This document can evolve through the time depending on the job market. 4 A job posting is composed of three elements: a job offer, a person speci&#64257;ca- tion and a set of applications. The job offer is the document that describes the job position (e.g., technician, researcher) but also which are the characteristics that are searched; this document is visible to the job seekers. The person speci&#64257;cation, see Footnote 3, is a document only accessible to the HRM and the employer. The set of applications corresponds to the r&#233;sum&#233;s and other documents that are propor- tioned by the job seekers interested in the job offer. 5 At the beginning of this work, none of the job postings was linked with its respective job offer. However, after a manual search, we arrived to manually link a portion of job postings, from the database, with their respective job offers. With this subset we created a baseline. techniques, but the main was that in Cabrera-Diego, Durette, Lafon, Torres-Moreno, and El-B&#232;ze (2015) we started to observe that r&#233;- sum&#233;s could be used to rank themselves using similarity measures. Thus, a simple method, like the one here presented could work. Moreover, by using methods based on neural networks, we reduce the chances of understanding and providing the reasons of why a candidate has been chosen to be interviewed, something that it is being looked for, like in Martinez-Gil, Paoletti, and Schewe (2016). The results obtained from applying our methods over a large set of real recruitment and selection processes, show that our meth- ods, despite not using job offers or semantic resources, can reach great performance. By just applying the method based on r&#233;sum&#233;s proximity, we can rank correctly in average 61% of the r&#233;sum&#233;s sent for a job posting. Nonetheless, this value can reach 93% when it is used along with our proposed Relevance Feedback methods, in which an HRM just need to analyze 20 r&#233;sum&#233;s per job posting, i.e., no more than 50% of the applications sent to the job posting. In summary, this work present multiple and diverse contribu- tions. The &#64257;rst contribution is that we offer an innovative method, completely different to the ones found in the state-of-the-art, that can rank r&#233;sum&#233;s correctly and automatically. Although this sys- tem is used in a very speci&#64257;c context, where job offers are not always present, it can be applied in any condition where the goal is to rank r&#233;sum&#233;s sent to the very same job posting. The second contribution is the use of two different Relevance Feedback that can improve to a great extent other r&#233;sum&#233; ranking systems. The third and &#64257;nal contribution is the methodology used in this arti- cle, which can be used by people to do a posteriori analyses of se- lection processes. For example, HRMs can use the methodology to understand how the selection of candidates was done and which were the keywords that represented the selected and rejected can- didates. As well, HRMs can use the tool to determine whether a candidate that should have been called for an interview was left aside. Whereas, psychologist can use the outcome of our methods as a way to determine whether HRM infers aspects like personality (Cole, Feild, Giles, &amp; Harris, 2009) or whether they are affected by errors like misspellings (Martin-Lacroux, 2017). In addition, other systems could use our methods&#8217; outputs to generate feedback that rejected candidates could &#64257;nd useful to improve their pro&#64257;les. This work is divided into eight sections. In Section 2, we introduce the state-of-the-art methods and our previous work. The methodology and the data are explained in Section 3 and Section 4, respectively. We introduce the experimental and eval- uative settings in Section 5. The outcomes from the experiments are presented in Section 6. We discuss the results in Section 7. The work&#8217;s conclusions and possible future work are presented in Section 8. 2. Related work In 2002, Harzallah, Lecl&#232;re, and Trichet (2002) presented the project CommOnCV that consists of an automatic analysis and matching of competencies between r&#233;sum&#233;s and job offers. To the best of our knowledge, this was the &#64257;rst project where the sci- enti&#64257;c community became interested in the automated analysis of r&#233;sum&#233;s. Since the publication of this project, several systems were developed with different approaches and goals. We have grouped the systems into three types: R&#233;sum&#233; matchers, R&#233;sum&#233; classi&#64257;ers and R&#233;sum&#233; rankers. R&#233;sum&#233; matchers are systems created for on-line job boards that match uploaded r&#233;sum&#233;s with a job offer or a query, e.g., Garc&#237;a-S&#225;nchez, Mart&#237;nez-B&#233;jar, Contreras, Fern&#225;ndez-Breis, and Castellanos-Nieves (2006); Guo, Alamudun, and Hammond (2016); Radevski and Trichet (2006); Sen, Das, Ghosh, and Ghosh (2012). To achieve the matching of r&#233;sum&#233;s, these systems use mainly on- L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 93 tologies and rules, but they can use some kind of Relevance Feed- back,6 like in Hutterer (2011) to improve the match results. R&#233;sum&#233; classi&#64257;ers consist of systems that bypass HRMs by au- tomatically classifying r&#233;sum&#233;s into relevant or irrelevant candi- dates. These kinds of systems, such as Kessler, Torres-Moreno, and El-B&#232;ze (2008b) and Faliagka et al. (2013), use machine learning methods to perform this task. In other words, they create a model using data from previous selection processes. The model contains, in theory, the features that make an applicant to appear relevant or irrelevant to an HRM. R&#233;sum&#233; rankers are systems that sort r&#233;sum&#233;s based on prox- imity between a r&#233;sum&#233; and a job offer, or even others r&#233;sum&#233;s. As these systems propose rankings, an HRM can decide the point in which r&#233;sum&#233;s become irrelevant for a job and stop reading them. In this kind of systems, proximity between elements can be lexical (Cabrera-Diego, 2015; Kessler, B&#233;chet, Roche, El-B&#232;ze, &amp; Torres-Moreno, 2008a; Singh et al., 2010), semantic (Kmail, Maree, &amp; Belkhatir, 2015; Montuschi et al., 2014; Tinelli, Colucci, Donini, Di Sciascio, &amp; Giannini, 2017) or ontological (Senthil Kumaran &amp; Sankar, 2013). In the following paragraphs we discuss the most representative r&#233;sum&#233; rankers found in the literature. E-Gen (Kessler et al., 2009) is a system that can create r&#233;- sum&#233; rankings based on the lexical proximity between r&#233;sum&#233;s and a particular job offer. More speci&#64257;cally, E-Gen compares r&#233;- sum&#233;s and a speci&#64257;c job offer using measures such as Cosine Sim- ilarity and Minkowski Distance. The r&#233;sum&#233;s are ranked accord- ing to how proximal they are to the job offer. The documents, i.e., r&#233;sum&#233;s and job offers, are represented using a Vector Space Model. As well, they make use of a Relevance Feedback method that consists of enriching the job offer vocabulary by concatenat- ing already analyzed relevant r&#233;sum&#233;s from the same job posting. In Kessler et al. (2012) the authors improved the system&#8217;s perfor- mance by adding an automatic text summarization tool to obtain the most relevant information from job offers and r&#233;sum&#233;s. PROSPECT is a system developed by Singh et al. (2010) that has a r&#233;sum&#233; ranker among its tools. PROSPECT extracts relevant information from r&#233;sum&#233;s and job offers using Conditional Ran- dom Fields (CRF), a lexicon, a named-entity recognizer and a data normalizer. Then, to rank the r&#233;sum&#233;s based on the job offer, PROSPECT compares the information from both documents using Okapi BM25, Kullback-Leibler Divergence or Lucene Scoring. We note in the literature the LO-MATCH platform (Montuschi et al., 2014). It is a web-based system developed to match professional competencies from r&#233;sum&#233;s and job offers. The LO-MATCH platform is based on ontologies which are used to enhance information from r&#233;sum&#233;s and job offers. The ranking of r&#233;sum&#233;s with respect to a job offer is determined through semantic similarity. LO-MATCH establishes to what degree the words found in a r&#233;sum&#233; have similar or related meanings to the words occurring in a job offer. The r&#233;sum&#233;s most similar to the job offer are ranked near the top. EXPERT (Senthil Kumaran &amp; Sankar, 2013) is another system that ranks r&#233;sum&#233;s. However, each r&#233;sum&#233; and job offer is individually represented by an ontology. To generate each ontology, EXPERT an- alyzes the information with an ontology and a set of previously de- &#64257;ned rules (Senthil Kumaran &amp; Sankar, 2012). EXPERT ranks the r&#233;- sum&#233;s by determining how close the job offer ontology is with re- spect to each r&#233;sum&#233; ontology. The r&#233;sum&#233;s with ontologies most similar to those of the job offer are ranked near the top. MatchingSem (Kmail et al., 2015) is a ranking system designed to use multiple ontologies to &#64257;nd the most similar r&#233;sum&#233;s for 6 Relevance Feedback is the interaction of a human user with an information re- trieval system, in order to evaluate its results and to modify requests for improving data retrieval Rocchio (1971). a job posting. The reason to design a system capable to extract information from multiple ontologies is to represent several do- mains and/or decrease their lack of coverage. Thanks to ontologies, MatchingSem can create semantic networks that are matched us- ing the Jaro-Winkler distance. I.M.P.A.K.T. (Tinelli et al., 2017) is a platform that allows HRM ranking candidates automatically and obtain the reasons of putting a r&#233;sum&#233; in a certain position. It is based on Relational database Management Systems which help in the creation of improved knowledge bases. As well, the platform allows de&#64257;ning which com- petencies are required and which are only desired. I.M.P.A.K.T. of- fers to HRMs information about con&#64258;icts or underspeci&#64257;ed features found in a r&#233;sum&#233;. Another r&#233;sum&#233; ranker is the one detailed in our previous work (Cabrera-Diego, 2015). There, we present the &#64257;rst version of the method that in this work is extended and improved. It consists of using a measure that we call Average Inter-R&#233;sum&#233; Proximity (AIRP). This measure determines the relevance of a r&#233;sum&#233; according to how similar it is to other r&#233;sum&#233;s from the same job posting. To improve the ranking of r&#233;sum&#233;s, we use Relevance Feedback and apply it with a factor that increases when a r&#233;sum&#233; is closer to those considered by an HRM as relevant. In the last years, some other researchers have worked on tasks related to the automatic ranking of r&#233;sum&#233;s. For example, in Martinez-Gil et al. (2016) the authors propose an approach to im- prove the ranking of r&#233;sum&#233;s by matching learning; as well, how to use matching learning to represent, in the future, documents using a common vocabulary. As well, related to the previous work, Martinez-Gil, Paoletti, R&#225;cz, Sali, and Schewe (2018) propose a the- ory of how to match r&#233;sum&#233;s and job offers, but also ranking them by using knowledge bases, lattice graphs and lattice &#64257;lters. Another example is the analysis of social media to evaluate the emotional intelligence of candidates (Menon &amp; Rahulnath, 2016). In Zaroor, Maree, and Sabha (2017), for instance, r&#233;sum&#233;s and job offers are classi&#64257;ed automatically in occupational categories; se- mantic networks are used to &#64257;nd the best matching between these documents. 3. Methodology Our methodology is composed of two parts. In the &#64257;rst part, we determine the similarity of r&#233;sum&#233;s in order to rank them. In the second, which is optional although suggested, we ask the HRM for Relevance Feedback and apply it. More speci&#64257;cally, the methodol- ogy used in this article is composed of &#64257;ve steps which are graph- ically represented in Fig. 1. In step I, we calculate the proximity between pairs of r&#233;sum&#233;s using Inter-R&#233;sum&#233; Proximity (Section 3.1). Once all the proximity values have been calculated, we estimate the Average or Median Inter-R&#233;sum&#233; Proximity for each r&#233;sum&#233; in step II (Section 3.2). It is in this step where we formulate the hypothesis that the result- ing values indicate the relevance of the r&#233;sum&#233;s for the job post- ing. In step III, we sort the scores obtained in step II in descending order to rank the r&#233;sum&#233;s. If we want to improve a ranking, we can make use of Rele- vance Feedback (Section 3.3). This process starts in step IV where an HRM analyzes a small set of r&#233;sum&#233;s in order to determine whether they are relevant or not. Furthermore, they can identify and sort the terms that represent better relevancy. Once the HRM has &#64257;nished, the Relevance Feedback is processed in step V. In this case, we can process the Relevance Feedback using the Relevance Factor (Section 3.3.1) and Vocabulary Scoring (Section 3.3.2). The output of the Relevance Feedback is then introduced in step III to re-rank the remaining r&#233;sum&#233;s, i.e those not seen during the Rele- vance Feedback. 94 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 Fig. 1. Methodology overview. 3.1. Inter-R&#233;sum&#233; Proximity The Inter-R&#233;sum&#233; Proximity (IRP) is de&#64257;ned as the degree of sim- ilarity between two r&#233;sum&#233;s that were sent by different candidates applying for the same job positing. To mathematically de&#64257;ne the IRP, consider J as the set of r&#233;sum&#233;s gathering all the candidates that applied to the same job posting, J = {r1, r2, r3, . . . rj}. Every r&#233;sum&#233; r in J is unique and from a different applicant, i.e. there are no duplicated r&#233;sum&#233;s or candidates in the job posting. We present the de&#64257;nition of Inter-R&#233;sum&#233; Proximity (IRP) by Eq. (1). IRP(r, rx) = &#963; (r, rx); &#8704;r \x04= rx; r, rx &#8712; J (1) where r and rx are two different r&#233;sum&#233;s from J; &#963; is a proximity measure. In this study, we use Dice&#8217;s Coe&#64259;cient as &#963; because in Cabrera- Diego et al. (2015) we observed, through statistical analyses, that this similarity measure is the most adequate for this task.7 Al- though Dice&#8217;s Coe&#64259;cient is frequently de&#64257;ned in terms of sets, as in Eq. (2), we have rede&#64257;ned it in Eq. (3) to be used in a vector representation. Dice&#8217;s Coe&#64259;cient(r, rx) = 2 &#183; |r &#8745; rx| |r| + |rx| (2) Dice&#8217;s Coe&#64259;cient(r, rx) = 2 &#183; \x02n i min(&#945;i, &#945;xi) \x02n i &#945;i + \x02n i &#945;xi (3) where r = {&#945;1, &#945;2, . . . , &#945;n} and rx = {&#945;x1, &#945;x2, . . . , &#945;xn} are vector representations of the r&#233;sum&#233;s r and rx respectively. Each vector has n dimensions and their components are expressed by &#945;; min is a function that outputs the smallest component between r and rx in each vector dimension. Note that Dice&#8217;s Coe&#64259;cient has a closed interval [0, 1], where 1 means that both documents are identical and 0 indicates they are completely different and have nothing in common. 7 Other measures tested in Cabrera-Diego et al. (2015) were Cosine Similarity, Jac- card&#8217;s Index, Manhattan distance and Euclidean distance. However, it was Dice&#8217;s Co- e&#64259;cient the one that presented the best performance in the analysis of r&#233;sum&#233;s. 3.2. Average and median Inter-R&#233;sum&#233; Proximity In Cabrera-Diego et al. (2015) we determined through a sta- tistical analysis that, on average, the similarity between relevant r&#233;sum&#233;s is greater than the similarity between irrelevant ones. Equally, we observed that relevant r&#233;sum&#233;s tend to be dissimilar to the group of irrelevant r&#233;sum&#233;s. From this outcome, we can in- fer that relevant r&#233;sum&#233;s should have multiple terms in common, while irrelevant r&#233;sum&#233;s should present a variety of terms that are not shared, either by other irrelevant r&#233;sum&#233;s or by the relevant ones. Based on this interpretation, we designed what we call the Average Inter-R&#233;sum&#233; Proximity (AIRP). It is a method of &#64257;nding rel- evant r&#233;sum&#233;s based on their proximity to other r&#233;sum&#233;s. The con- cept is that a relevant r&#233;sum&#233; will have, on average, higher values of IRP than an irrelevant r&#233;sum&#233;.8 The mathematical de&#64257;nition of AIRP is presented by Eq. (4). AIRP(r) = 1 j &#8722; 1 j \x03 x=1 IRP(r, rx) (4) where r is a r&#233;sum&#233; selected for analysis from J, rx is another r&#233;- sum&#233; related to J but different from r and j is the number of r&#233;- sum&#233;s sent to J. We introduce as well the Median Inter-R&#233;sum&#233; Proximity (MIRP). It is a variation of AIRP, but it consists of calculating the median instead of the average of a set of Inter-R&#233;sum&#233; Proximity values. The main reason to use this central-tendency measure is that it is more robust against skewness and outliers9 than the mean. The formula for calculating the MIRP is given by Eq. (5). MIRP(r) = MEDIAN[IRP(r, rx)] j x=1 (5) 8 A relevant r&#233;sum&#233; should have high values of IRP with respect to other relevant r&#233;sum&#233;s and low values of IRP with respect to irrelevant ones. However, irrelevant r&#233;sum&#233;s should have constantly low values of IRP in accordance with the analyses done in Cabrera-Diego et al. (2015). 9 An outlier is a value with an atypical magnitude with respect to the total set (Mason, Gunst, and Hess, 2003, page 70). L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 95 where r and rx are two different r&#233;sum&#233;s from J and j is the num- ber of r&#233;sum&#233;s sent to J. 3.3. Relevance Feedback In addition to AIRP and MIRP, we propose to use Relevance Feedback as a method for validating and enriching the information used by our ranking methods. In our study, Relevance Feedback is the process where an HRM determines which r&#233;sum&#233;s, from a sample of the ranking given by AIRP or MIRP, are relevant and irrelevant for the job posting. Furthermore, an HRM can indicate the terms that better character- ize the relevant and irrelevant r&#233;sum&#233;s found during the previous step. Based on these inputs, we process and apply the feedback to offer an improved ranking of the remaining r&#233;sum&#233;s. The Rele- vance Feedback given for one job posting does not affect the way we rank other job postings, as the inputs can differ. We propose two methods for applying Relevance Feedback. The &#64257;rst method, called Relevance Factor and presented in Section 3.3.1, consists of calculating a quotient that takes into ac- count the Inter-R&#233;sum&#233; Proximity between a r&#233;sum&#233; and those considered relevant or irrelevant during Relevance Feedback. This method, as seen in Fig. 1, is introduced into the ranking process by a simple multiplication during the calculation of either AIRP or MIRP. The second method (Section 3.3.2) resides in weighting the terms indicated by the HRM that better represent the relevant and irrelevant r&#233;sum&#233;s seen during Relevance Feedback. Because of its characteristics, explained in its respective section, this last method modi&#64257;es the Relevance Factor. 3.3.1. Relevance factor The &#64257;rst method for introducing Relevance Feedback consists of determining the proximity between the remaining r&#233;sum&#233;s from a job posting and those, from the same job posting, that were ana- lyzed during the Relevance Feedback. We achieve this with a for- mula that we have called Relevance Factor (RFa). The Relevance Fac- tor goal is to improve the ranking of r&#233;sum&#233;s. Thus, on one hand, the Relevance Factor pushes to the ranking&#8217;s top the r&#233;sum&#233;s that are more proximal to those considered as relevant during the Rele- vance Feedback. On the other hand, it pulls down, to the ranking&#8217;s bottom, those r&#233;sum&#233;s which are more proximal to the irrelevant ones. Let us consider F = {r1, r2, . . . , rf } as the set of r&#233;sum&#233;s sent by applicants for a job posting J that were analyzed during a Rele- vance Feedback process. Each r&#233;sum&#233; from F was classi&#64257;ed by an HRM into one class, either relevant (R) or irrelevant (I). We have de&#64257;ned the Relevance Factor, RFa, in Eq. (6). RFa(r) = \x04 + \x02 IRP(r, rxR) \x04 + |R| &#183; \x04 + |I| \x04 + \x02 IRP(r, rxI) ; &#8704;rxR &#8712; R; rxI &#8712; I; rxR, rxI &#8712; F (6) where r is the r&#233;sum&#233; to be analyzed, R and I represent the set of r&#233;sum&#233;s considered, respectively, as relevant and irrelevant dur- ing the Relevance Feedback process. Furthermore, \x04 is a constant, empirically set to 1 &#215; 10&#8722;10 which is used to avoid undetermined values10 and IRP is the function described in Eq. (1). The behavior of the Relevance Factor depends on the interval of the proximity measure used to determine IRP (Eq. (1)). Since we use Dice&#8217;s Coe&#64259;cient, the Relevance Factor will be greater than one (RFa(r) &gt; 1) when the r&#233;sum&#233; r is more proximal to the rel- evant r&#233;sum&#233;s. It is going to be RFa(r) = 1 if r is equally similar 10 In some cases during the Relevance Feedback, it is possible to &#64257;nd only relevant or irrelevant r&#233;sum&#233;s, but not both. Without this constant one side of the formula would be 0/0. Table 1 Example of how the Relevance Factor would be calculated for three r&#233;sum&#233;s, A, B and C, that belong to a hypothetical job posting J containing eight different r&#233;sum&#233;s, J = {R1, R2, R3, I1, I2, A, B,C}. The example considers that J has three rel- evant r&#233;sum&#233;s (R1, R2, R3) and two irrelevant ones (I1, I2) previously detected by an HRM during a Relevance Feedback process. r rxR IRP(r, rxR) \x05 IRP(r, rxR) rxI IRP(r, rxI) \x05 IRP(r, rxI) RFa(r) A R1 0.90 2.45 I1 0.20 0.50 2.45 3 &#183; 2 0.50 = 3.26 R2 0.75 I2 0.30 R3 0.80 B R1 0.35 1.35 I1 0.40 0.90 1.35 3 &#183; 2 0.90 = 1.00 R2 0.55 I2 0.50 R3 0.45 C R1 0.30 0.90 I1 0.80 1.55 0.90 3 &#183; 2 1.55 = 0.38 R2 0.40 I2 0.75 R3 0.20 to relevant and irrelevant r&#233;sum&#233;s. And, if the r&#233;sum&#233; r has more in common with the irrelevant r&#233;sum&#233;s, the Relevance Factor will approach to zero. The introduction of the Relevance Factor into the ranking of r&#233;- sum&#233;s is done by simple multiplication, i.e., the Relevance Factor of a r&#233;sum&#233; is multiplied by its respective score determined by either AIRP or MIRP. To understand the Relevance Factor better, it should be indi- cated that Eq. (6), can be split into two parts. The left side calcu- lates IRP with respect to the relevant r&#233;sum&#233;s, while the right side is in accordance with the irrelevant r&#233;sum&#233;s. We describe in the following paragraph a hypothetical process of its calculation. Let us consider a job posting J composed of eight differ- ent r&#233;sum&#233;s, J = {R1, R2, R3, I1, I2, A, B,C}. During a Relevance Feed- back process, an HRM analyzed &#64257;ve of these r&#233;sum&#233;s, i.e., F = {R1, R2, R3, I1, I2}, and found out that three were relevant (R1, R2, R3), while two were irrelevant (I1, I2). In Table 1, we present how the Relevance Factor would be calculated for the r&#233;sum&#233;s that were not analyzed by the HRM (A, B, C). As it can be observed in Table 1, the r&#233;sum&#233; A is very similar to relevant r&#233;sum&#233;s, there- fore, its RFa(A) = 3.26; this means that its score, either AIRP or MIRP, will be multiplied by a factor of 3.26. Regarding r&#233;sum&#233; B, it has a RFa(B) = 1.00, this means that it is equally similar to rel- evant and irrelevant r&#233;sum&#233;s; the AIRP or MIRP of B will rest the same. Concerning r&#233;sum&#233; C, it has a RFa(C) = 0.38 due to its high similarity to irrelevant r&#233;sum&#233;s and, in consequence, its AIRP or MIRP will be affected by a factor of 0.38. 3.3.2. Vocabulary Scoring The second method for applying Relevance Feedback consists of processing the vocabulary that, in accordance with the HRM, bet- ter represents the r&#233;sum&#233;s marked as relevant or irrelevant during the Relevance Feedback. The objective is to adjust the weights of the terms that cause a candidate to be considered by an HRM as relevant or irrelevant for the job posting. To achieve this, during the Relevance Feedback an HRM indicates and sorts which terms, seen in the analyzed r&#233;sum&#233;s, characterized what made a candi- date to be relevant or irrelevant. The sorting of the terms should be done regarding their representativeness. Formally, consider Vc = {t1,t2, . . . ,tv} as the vocabulary selected and sorted by an HRM that better represents the r&#233;sum&#233;s from class c during Relevance Feedback. For each term from Vc, we com- pute its Term Score Tc(t), i.e., a value that allows us to boost or minimize the terms that de&#64257;ne each class c. In Eq. (7), we de&#64257;ne the Term Score Tc(t) for a term t appearing in Vc. Tc(t) = 5 \x04 1 rank(t) ; &#8704;t &#8712; Vc (7) where rank(t) is the position of term t de&#64257;ned by an HRM in Vc. The Term Score always has a value within the half-closed interval 96 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 Rank(t) Term Score (t) Fig. 2. Plot of the term score for ranks of t between 1 and 50. [1, 0). A term with a value close to 1 expresses a high representa- tiveness of class c, while a term with a value near to zero means that it hardly represents class c and should be minimized. Using a root in Eq. (7), speci&#64257;cally the 5th root, should be dis- cussed. We empirically chose this function for two reasons. First, it allows us to create a score between 1 and 0. Second, it slowly decreases and preserves the sense of representativeness provided by the HRM, i.e., the way that terms were sorted by the HRM is kept. It can be seen in Fig. 2 how the Term Score changes in accor- dance to the rank of t; for instance, the term that is ranked &#64257;rst has a Term score equal to 1; the second ranked term has a score Tc = 0.870; and for the &#64257;ftieth score, Tc = 0.457. As there is always a set of terms that will not appear in V but that are found in other r&#233;sum&#233;s for the same job posting J, it is essential to give to these terms a Term Score Tc in order to keep the model balanced. In other words, we cannot leave the terms that did not appear in a Vc with higher values than those that were analyzed by an HRM. We assign a value of 0.01 to all the terms belonging to the r&#233;sum&#233;s of J that are not present in a Vc.11 This &#64257;gure was chosen empirically to minimize the terms that are not representative of the model without deleting them.12 Since the Term Score Tc(t) of every term t is different for each class c (relevant and irrelevant), we use the Term Score uniquely within the Relevance Factor (Section 3.3.1), as it calculates the Inter-R&#233;sum&#233; Proximity with respect to relevant and irrelevant r&#233;- sum&#233;s separately. To be more speci&#64257;c, the Term Scores only mod- ify terms&#8217; weights of each class used at the computation of Inter- R&#233;sum&#233; Proximity in Eq. (6). 3.3.3. Selecting the r&#233;sum&#233; s for the Relevance Feedback Even though the Relevance Feedback described in Rocchio (1971) consists of choosing a number of top-retrieved documents, we test whether the Relevance Feedback determined with other position-retrieved documents is useful to HRMs. More speci&#64257;cally, we use the Relevance Feedback of the documents retrieved from the following positions: &#8226; Top. This is the classic method which consists of taking the top ranked r&#233;sum&#233;s to improve the following rankings. In the case 11 For instance, a term t can appear in VIrrelevant but not in VRelevant. Thus, for this same t the Term score VRelevant will be 0.01. 12 We experimented with other values: 0.25, 0.1 and 0.05. We observed that by decreasing the value the results were improved. where we &#64257;nd non-relevant r&#233;sum&#233;s among the top, it may be a way to determine which characteristics, although common, may not be required for the job or are not the ones searched by the HRM. &#8226; Bottom. This is the opposite of the classic method, as we se- lect the r&#233;sum&#233;s located at the end of rankings. We infer that &#64257;nding a relevant r&#233;sum&#233; with a low ranking can provide more useful feedback than detecting an irrelevant r&#233;sum&#233; at the top. Furthermore, this may be interesting for the Human Resources domain, as leaving a relevant r&#233;sum&#233; at the bottom would set aside the objectives of the rankings. &#8226; Top and Bottom (henceforth Both). For this position, we de- cided to merge the ideas from the &#64257;rst two Relevance Feedback positions. More speci&#64257;cally, in this position we ask a recruiter whether some r&#233;sum&#233;s from the top and the bottom are truly relevant.13 The goal is to reduce the weaknesses of the Bottom and Top positions; we may detect truly relevant and irrelevant documents ranked &#64257;rst and also those that are interesting but mis-positioned at the end of a ranking. In addition to the different Relevance Feedback positions, we decided to test whether an iterative application of Relevance Feed- back could improve the r&#233;sum&#233; rankings more quickly. Under non- iterative conditions, once the Relevance Feedback has produced a new ranking the process ends. Nonetheless, for iterative conditions, once a new ranking is produced, it can be re-analyzed by an HRM in a new Relevance Feedback process. 4. Data For this article, we used a set of 171 job postings which were processed (recruitment and selection) by a French human resources enterprise between November 2008 and March 2014. These job postings come from different professional domains (e.g., chemistry, communications, physics and biotechnology) and posi- tion levels (e.g., laboratory researcher, intern, project manager and engineer). These 171 job postings were chosen because they con- tain at least 20 unique r&#233;sum&#233;s in French; at least 5 of them are 13 Half of the r&#233;sum&#233;s for the Relevance Feedback are from the top. The other half belong to the ranking&#8217;s bottom. L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 97 relevant, and 5 are irrelevant.14 In total, the corpus contains 14,144 French r&#233;sum&#233;s divided among these 171 job postings. All the job postings are composed of applications, and each ap- plication contains the documents associated with the recruitment and selection process. It is important to note that not all the doc- uments located inside the applications corresponded to r&#233;sum&#233;s; we could &#64257;nd motivation and recommendation letters, diplomas, interview minutes and social network invitations as well. To ob- tain only the French r&#233;sum&#233;s, we made use of a r&#233;sum&#233; detec- tor. The r&#233;sum&#233; detector is a linear Support Vector Machine (SVM) developed previously in Cabrera-Diego et al. (2015). Furthermore, all the r&#233;sum&#233;s were lower cased and lemmatized; for lemmatiz- ing the documents, we used Freeling 3 (Padr&#243; &amp; Stanilovsky, 2012). Stop-words, punctuation marks and numbers were deleted. In ad- dition, all duplicated r&#233;sum&#233;s within the same job posting were deleted.15 See Cabrera-Diego et al. (2015) in order to learn more about this pre-processing task. According to the HRMs with whom we worked, the system em- ployed to manage the applications allowed them to organize each applicant into one of the following selection phases: Unread, Ana- lyzed, Contacted, Interviewed and Hired. The phases were assigned to each applicant depending on the last point to which they ar- rived. For this article, we grouped four of these phases into two different classes: relevant and irrelevant. The &#64257;rst class, relevant, corresponds to the phases Contacted, Interviewed and Hired. It represents the applicants who after read- ing their r&#233;sum&#233;s were approached by a recruiter. The second class, irrelevant, contains only the r&#233;sum&#233;s that remained in the Analyzed phase, i.e., the applicants that were not approached by an HRM after reading their r&#233;sum&#233;s. With respect to the applications that remained in the Unread phase, these were discarded from the analysis since we cannot in- fer whether they were relevant or irrelevant for the job. Further- more, most of these applications were not read because the selec- tion process ended as they were received. There are two reasons to classify four of &#64257;ve phases into two classes. The &#64257;rst is that to determine whether an applicant will be hired implies the analysis of elements that are not present in a r&#233;sum&#233; (e.g., interview results, expected salary, job location and withdrawal). The second one is that we do not want to replace hu- mans with an automaton in the selection process. Instead, we want to assist humans during the most di&#64259;cult part of the selection pro- cess, which is in discerning relevant and irrelevant applicants. And this can be achieved by ordering applicant&#8217;s r&#233;sum&#233;s in terms of how relevant are for the job posting. It is important to note that some applications from the cor- pus, although impossible to trace, started as Direct contact. This means that an HRM found, usually on the Internet or job seek- ers databases, the r&#233;sum&#233; of a person who ful&#64257;lled the person speci&#64257;cation and decided to contact this person directly. Thus, for some job postings the relevant r&#233;sum&#233;s can accurately re&#64258;ect the searched pro&#64257;le. This action can affect the number of relevant ap- plicants for a job posting, which in some cases can be equal or greater than the number of irrelevant applicants. However, this characteristic from the corpus should be seen as normal, since for an HRM to make direct contact is a way to speed up the recruit- ment and selection processes. 14 All the r&#233;sum&#233;s must be either relevant or irrelevant, but each job cannot have less than 5 per class. 15 There were job postings in which the same applicant sent their own r&#233;sum&#233; multiple times. Thus, to avoid a bias, we deleted the duplicated r&#233;sum&#233;s with a set of heuristics developed in Cabrera-Diego et al. (2015). Among the heuristics used, we can highlight the selection of the most recent r&#233;sum&#233; in the application folder or the detection of the exact same applicant e-mail. Furthermore, it should be indicated that we do not combine ap- plications from different job postings, even if they belong to sim- ilar job positions. The reason is that each job posting is linked to a job offered by a speci&#64257;c enterprise, in a particular date and with a set of desired characteristics. In other words, each job post- ing might attract different job seekers despite describing a very similar job position; aspects like years of experience, spoken lan- guages, mobility, relocation and salary can affect how the job mar- ket reacts. This variability makes impossible to determine whether a candidate from one job posting would participate in another one or whether a candidate would be considered equally relevant. To conclude with this section, after a manual search, we arrived to link 60 of the 171 job postings with their respective job offer. With these 60 job postings we created a baseline that will be de- scribed in Section 5. 4.1. Data representation We decided to represent each r&#233;sum&#233; from the corpus as a set of n-grams in a Vector Space Model (VSM) (Salton, Wong, &amp; Yang, 1975). To be speci&#64257;c, for each r&#233;sum&#233; we extracted its set of un- igrams, bigrams and trigrams. Every set of n-grams was saved as a vector, one per r&#233;sum&#233;. The vectors&#8217; component weights (W) are the relative frequency of each n-gram which could be multiplied by a weight modi&#64257;er (\x06); we present W in Eq. (8). W (&#8226;) = F(&#8226;) &#183; \x06(&#8226;) (8) where &#8226; is an n-gram and F is the relative frequency calculated with respect to each r&#233;sum&#233;. The weight modi&#64257;er \x06 can be one of the following: &#8226; \x06 = 1. In this case, we represent the data only by the relative frequency of each n-gram. &#8226; \x06 = IDF(&#8226;). Each n-gram (&#8226;) is weighted with respect to a Term-Frequency Inverse-Document Frequency (TF-IDF) Sp&#228;rck- Jones (1972).16 Once the r&#233;sum&#233;s of a job posting have been ranked for the &#64257;rst time, either with AIRP or MIRP, and a vocabulary scoring has been set, a new \x06 for Eq. (6) can be used: &#8226; \x06 = Tc(&#8226;). In this case each n-gram (&#8226;) is modi&#64257;ed by its re- spective Term Score Tc (see Eq. (7)); where c is the class (rele- vant or irrelevant) that will affect uniquely. &#8226; \x06 = IDF(&#8226;) &#183; Tc(&#8226;). It is similar to the previous \x06, however, it can be modi&#64257;ed by IDF in the case, the original representation made use of the weight too. In all the cases, these last two \x06 do not affect permanently the weights of the terms, they are only locally used each time Eq. (6) is called. 4.2. Data for the Relevance Feedback Although the ideal experimentation would consist in applying our methods and asking HRM for Relevance Feedback on real time, the fact is that this task would be very expensive. Moreover, the HRM would have to do this task besides their normal work duties and it would be hard to get accurate results in cases where the person speci&#64257;cation evolved over time. Thus, we decided to simu- late the Relevance Feedback. Regarding the Relevance Feedback in which an HRM indicates whether a r&#233;sum&#233; is relevant or irrelevant, we made use of the information available in the corpus. As we explained at the begin- ning of Section 4, every application and, therefore, every r&#233;sum&#233; 16 The IDF for each unigram, bigram and trigram was calculated using all the cor- pus described at the beginning of Section 4 (14,144 r&#233;sum&#233;s). 98 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 belongs to a real selection process. Thus, at a given moment, every r&#233;sum&#233; was analyzed by an HRM who considered whether it was from a relevant or irrelevant applicant. The information found in the corpus allows us to create a simulation that can be reproduced again if necessary. For the vocabulary scoring, we decided to explore three simu- lations, S1, S2 and S3, in which we select and weight differently the terms for the vocabulary scoring. Each simulation is composed of 100 n-grams in total, 50 describing the relevant r&#233;sum&#233;s and 50 the irrelevant ones. These simulations are different from the one used to determine whether a r&#233;sum&#233; is relevant or irrelevant, as the corpus does not contain this kind of information. However, they are based on information found in the corpus and in con- sequence reproducible. In the following subsection, we explain in detail how S1, S2 and S3 were determined. 4.2.1. Simulations for Vocabulary Scoring Consider V = {t1,t2, . . . ,tv}, the vocabulary composed of the n- grams (t) that occur in at least 2 r&#233;sum&#233;s from the Relevance Feed- back.17 The process to generate the three simulations is as follows: 1. For each term t belonging to V, we calculate the squared proba- bility of term t occurring in each possible class c, either relevant or irrelevant. This is done using Eq. (9): p2 c (t) = \x05 Dc(t) D(t) \x062 (9) where Dc(t) is the number of r&#233;sum&#233;s belonging to class c; D(t) is the number of r&#233;sum&#233;s analyzed in the Relevance Feedback. The equation is an adaptation of Gini&#8217;s Coe&#64259;cient18 presented in Cossu (2015). For a set of classes C = {c1, c2, . . . , ck}, Gini&#8217;s Coe&#64259;cient has an interval between [1/k, 1], where 1/k means that a term appears in every class, while 1 indicates that the term belongs to one class (Torres-Moreno et al., 2012). 2. Then, we calculate a factor (fc) that takes into account the num- ber of documents from class c where the n-gram appeared and the sum of the n-gram&#8217;s weights (W) inside these documents. The factor is presented in Eq. (10). fc(t) = Dc(t) &#183; \x03 Wc(t) (10) where t represents an n-gram, c is one of the two possible classes (relevant or irrelevant), fc is the factor for the class c, Dc is the number of documents of class c where t appears and W is the n-gram weight (see Eq. (8)). 3. For each class c we sort the n-grams &#64257;rst according to their squared probabilities p2 c and then by their factor fc. If two or more n-grams share the same squared probabilities and factors, although this is unusual, we assign them different but consec- utive locations in the sorted list. 4. We select the &#64257;rst 50 n-grams for each class, to which we cal- culate their Term Scores (Tc) using Eq. (7). 5. For the rest of n-grams, or those that did not occur in the r&#233;sum&#233;s from the Relevance Feedback, we give them a Term Score of 0.01 as explained in Section 3.3.2. Simulation S1 consists of selecting and scoring the vocabulary according to the information found only in the r&#233;sum&#233;s used for 17 Because we simulate the vocabulary scoring, to use terms that were seen only in one r&#233;sum&#233; may not be reliable but speculative. In fact, a one time-seen term, and in consequence its pertinence, may be no more than a coincidence which could change by increasing the number of documents analyzed. 18 Although Gini&#8217;s Coe&#64259;cient is frequently used in economics for wealth dis- tribution, it has been used in other NLP works, e.g., Fang and Zhan (2015) and Cossu, Janod, Ferreira, Gaillard, and El-B&#232;ze (2014). Gini&#8217;s Coe&#64259;cient in NLP has the objective of modifying the weight of an element in the data model by determining to which degree it represents a certain class or set of them (Torres-Moreno, El-B&#232;ze, Bellot, &amp; B&#233;chet, 2012). Relevance Feedback. In other words, the r&#233;sum&#233;s from the Rele- vance Feedback are used to calculate the squared probabilities and the factors of the n-grams. Next, for each class c, we calculate the Term Scores for the &#64257;rst sorted 50 n-grams. For simulation S2, we decided to recreate a scenario where the selection and sorting of the terms is done carelessly. Put differ- ently, the terms that, in theory, represent relevant and irrelevant r&#233;sum&#233;s are ignored and are not used in the Relevance Factor (Eq. (6)). To this end, we sort the n-grams using only the informa- tion from the Relevance Feedback, as we do for S1, but the Term Score of the &#64257;rst 50 n-gram of each class c is set to zero (Tc = 0).19 For the rest of terms, the Term Score is the default one, i.e. 0.01. In simulation S3, we try to model optimally the n-grams that would be chosen by an HRM in real life. To this end, we calculate the squared probabilities and factors fc based on the information in all the r&#233;sum&#233;s from the job posting. However, we continue to sort and calculate the Term Scores for the terms that only occur in the r&#233;sum&#233;s from Relevance Feedback. In summary, we can have high reliable squared probabilities and factors fc but we only affect the n-grams that would have been seen by an HRM during the Rel- evance Feedback.20 5. Experimental and evaluative settings There are multiple experiments that can be done following different con&#64257;gurations, however, although we explored a large amount of possible combinations, due to space limitations we only present the experiments that could contribute the most to the state-of-the-art. The experiments realized are summarized in the following list: &#8226; No Relevance Feedback: We apply our methods without using any kind of relevance feedback, and we compare them against a couple of baselines. &#8226; Relevance Feedback applied using &#8211; The Relevance Factor: We explore how different Relevance Feedback position (Top, Bottom and Both) affect the Rel- evance Factor. As well, we analyze whether the iterative application of the Relevance Factor can improve faster the ranking of r&#233;sum&#233;s. &#8211; The Relevance Factor with Vocabulary Scoring: We analyze how the simulations of Vocabulary Scoring affect the rank- ings created by the Relevance Factor. Two different baselines are used, the &#64257;rst one consists of a sys- tem that generates a random ranking for each job posting. The sec- ond baseline resides in using the 60 job postings to which we ar- rived to link with their respective job offer and calculate the sim- ilarity r&#233;sum&#233;s/job offer. More speci&#64257;cally, for each job posting, we apply Dice&#8217;s Coe&#64259;cient between its job offer and every ele- ment from its set of r&#233;sum&#233;s. Job offers are pre-processed under the same parameters that the r&#233;sum&#233;s, as explained in Section 4. Although a comparison with other methods or systems from the state-of-the-art would have been desired, to the extent of our 19 By making zero the Term Score of these n-grams, we affect their weight in the vector space model as explained in Section 4.1. This modi&#64257;cation has, in conse- quence, an effect in the Relevance Factor (Eq. (6)), where the r&#233;sum&#233;s containing most of the terms representing a class, instead of being pushed up or pulled down, they will stay in the same position in the rank. 20 In simulation S3 is possible that after sorting the n-grams, the one placed in the &#64257;rst place does not appear in the Relevance Feedback. Thus, as this n-gram could not have been seen by the HRM during the Relevance Feedback, we must consider another n-gram as the one in the &#64257;rst place. This will be the &#64257;rst term seen in the Relevance Feedback that has the best squared probability and factor fc. For the following terms the rules are the same. L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 99 knowledge, none of the systems or datasets have been released to the public.21 In the case of the experiments with Relevance Feedback, we have restricted the feedback size to a range between 2 and 20 r&#233;- sum&#233;s. It should be noted that we never use more than 50% of the r&#233;sum&#233;s for each job as feedback. In fact, the 171 job postings de- scribed in Section 4 were chosen because they had at least 20 r&#233;- sum&#233;s, from which at least 5 were from relevant applicants and 5 from irrelevant ones. When we use more than 10 r&#233;sum&#233;s for the Relevance Feedback, we always verify that there is at least twice the r&#233;sum&#233;s for the job posting, with more than 1/4 of them being relevant and no less than 1/4 irrelevant. For example, to do a Rel- evance Feedback of 16 r&#233;sum&#233;s, a job posting must have at least 32 r&#233;sum&#233;s in total, and no less than 8 must be relevant or irrele- vant. If one job posting do not have these characteristics then it is discarded, for that size of Relevant Feedback, from the analysis. All these precautions are taken to avoid in&#64258;ating the measurements for evaluation arti&#64257;cially. In the experiments related to the iterative application of Rele- vance Factor, we explore how rankings are affected when multi- ple and sequential Relevance Factor processes are done. In other words, we start by doing a Relevance Factor over 2 r&#233;sum&#233;s. This process will rank the remaining r&#233;sum&#233;s of the job posting in an improved way. After that, a new process of Relevance Feedback is done on which 2 new r&#233;sum&#233;s are analyzed. The Relevance Fac- tor is calculated again and the process is repeated until having re- vealed up to 20 r&#233;sum&#233;s. It should be mentioned that the corpus had 171 job postings that ful&#64257;lled the characteristics used for the Relevance Feedback up to size 10. For a Relevance Feedback of size 20, there were only 127 job postings with the established characteristics. All the calculations for AIRP and MIRP were parallelized using GNU Parallel (Tange, 2011), a shell tool created to run the same task multiple times but with different inputs. More speci&#64257;cally, the parallelization consists in assigning a CPU thread to each job post- ing. Therefore, multiple job postings can be run at the same time. We decided to evaluate each ranking of r&#233;sum&#233;s using Average Precision (AP) (Buckley &amp; Voorhees, 2000). AP is an evaluation met- ric designed for rankings with two grades of relevance: relevant and irrelevant.22 Furthermore, AP determines, at the same time, the precision and the recall of a ranking in accordance to the po- sition of its elements (Voorhees &amp; Harman, 2001). In order to have a good value of AP, i.e., close to 1, the relevant elements should be positioned at the top of a ranking, while those that are irrele- vant should be located at the bottom of a ranking. In our case, a ranked r&#233;sum&#233; is considered to have the correct relevance when it is similarly marked in the corpus data (see Section 4). To evaluate the performance of the methods used to rank r&#233;- sum&#233;s, we calculate the Mean Average Precision (MAP) for each one (Buckley &amp; Voorhees, 2000). As the name indicates, the MAP consists of averaging all the AP values obtained using the same method. In order to verify whether the MAP values obtained for each tested method are signi&#64257;cantly different, we analyze the results us- ing a one-way Repeated Measures Analysis of Variance (rANOVA). The assumptions of rANOVA, data normality and sphericity, are tested with the Shapiro-Wilk Test and the Mauchly&#8217;s Test, respec- 21 The only exception could be LO-MATCH, which provided a service through a website during a time. However, the software, per se, was never available to down- load for testing purposes. 22 Apart from the AP, we can &#64257;nd in the literature two other metrics specialized in the evaluation of rankings: Kendall&#8217;s tau and (Normalized) Discounted Cumulative Gain (J&#228;rvelin &amp; Kek&#228;l&#228;inen, 2000). These metrics are used in rankings with mul- tiple grades of relevance, e.g., very relevant, relevant, irrelevant and very irrelevant. However, our data set is only annotated with two grades of relevance, thus, AP is the most appropriate metric. Table 2 Summary of the statistical analysis done over the results presented in Fig. 3. The upper diagonal shows the p value of the results that were signi&#64257;cantly different. The lower diagonal shows the values of Cohen&#8217;s d effect size. AIRP AIRP IDF MIRP MIRP IDF Random AIRP 0.017 - - 4.4 &#215; 10&#8722;4 AIRP IDF 0.230 - - 1.2 &#215; 10&#8722;3 MIRP - - - 5.4 &#215; 10&#8722;4 MIRP IDF - - - 1.5 &#215; 10&#8722;4 Random 0.316 0.344 0.309 0.339 tively. In both cases, the alpha to refute the null hypothesis is set to 0.05. The results from the rANOVA are considered to be signi&#64257;cantly different when the p value is less than 0.05. In the case we com- pare more than two methods, and the rANOVA show a signi&#64257;cant difference, we also make use of a post hoc test. More speci&#64257;cally, we utilize a Pairwise t-Test with &#945; = 0.05 in order to determine which pairs of groups are signi&#64257;cantly different. For each pair of experiments showing a signi&#64257;cant difference, we calculated the effect size using Cohen&#8217;s d. Effect sizes are values that helps to quantify the difference between two analyzed groups. As thumb rule, effect size can be classi&#64257;ed into small (d = 0.2), medium (d = 0.5) and large (d = 0.8) (Cohen, 1988, Page 20). The statistical analyses were performed using R (R Core Team, 2018). 6. Results In this section, we present the results regarding the experi- ments de&#64257;ned in Section 5. Every result presented in a graph in- cludes its respective 95% con&#64257;dence interval. 6.1. Experiments with No Relevance Feedback In Fig. 3 we present the results of AIRP and MIRP with and without the Inverse-Document Frequency (IDF). We also compare the results with respect to the random baseline. As it can be seen in Fig. 3, all the methods presented in this work surpass the value given by the random baseline. Nonetheless, AIRP and MIRP get similar MAP values. The corresponding rANOVA between the results presented in Fig. 3 indicates that there is a signi&#64257;cant difference between the results (p value = 2.153 &#215; 10&#8722;5). According to the post hoc test all the methods are signi&#64257;cantly different with respect to the ran- dom baseline (p value &lt; 0.001). Moreover, AIRP with IDF is signif- icantly different to AIRP (p value = 0.017). For the remaining pairs of methods, there is no statistical difference. The average effect size between our methods with respect to the random baseline is d = 0.327, which is medium-small. The effect size between AIRP and AIRP with IDF is d = 0.230. In Table 2, we present a summary of the results from the statistical test. In Fig. 4, we compare again our methods with respect to a ran- dom baseline but also with the one based on the similarity be- tween job offers and r&#233;sum&#233;s. This experiment was done uniquely over the corpus&#8217; subset composed of 60 job posting for which we had found their respective job offers (see Section 4). As shown in Fig. 4, our methods rank the r&#233;sum&#233;s better than methods using the similarity between job offers and r&#233;sum&#233;s. Moreover, our methods work better on these 60 job postings than with the complete set of 171. The reasons for these results will be discussed in Section 7. The rANOVA performed on the results shown in Fig. 4 indi- cates that there was a signi&#64257;cant difference between the meth- 100 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 Fig. 3. Results, in terms of the MAP, for the random baseline, AIRP and MIRP without applying any kind of Relevance Feedback. Fig. 4. Comparison of our methods and two baselines (random and similarity between job offer and r&#233;sum&#233;s) for 60 job postings. The values are presented in terms of the MAP, and we did not use any kind of Relevance Feedback. Table 3 Summary of the statistical analysis done over the results presented in Fig. 4, which correspond to the subset of 60 job postings. The upper diagonal shows the p value of the results that were signi&#64257;cantly different. The lower diagonal shows the values of Cohen&#8217;s d effect size. AIRP AIRP IDF MIRP MIRP IDF Job Offer/R&#233;sum&#233; Random AIRP 0.045 - - 7.8 &#215; 10&#8722;7 3.6 &#215; 10&#8722;5 AIRP IDF 0.357 - - 1.5 &#215; 10&#8722;9 7.8 &#215; 10&#8722;6 MIRP - - - 1.2 &#215; 10&#8722;6 4.7 &#215; 10&#8722;5 MIRP IDF - - - 4.1 &#215; 10&#8722;9 1.1 &#215; 10&#8722;5 Job Offer/R&#233;sum&#233; 0.800 1.012 0.784 0.977 0.025 Random 0.656 0.716 0.701 0.642 0.392 ods (p value = 3.270 &#215; 10&#8722;7). In fact, and in accordance with post hoc test, the method based on the similarity of job offer/r&#233;sum&#233; is signi&#64257;cantly different than the random baseline and all our meth- ods (p value &lt; 0.05). The effect size between the methods AIRP IDF, MIRP and MIRP IDF, and the job offer/r&#233;sum&#233; baseline is always d &gt; 0.780, which correspond to large effect sizes. In Table 3, we present a summary of the results from the statistical test. 6.2. Experiments with Relevance Feedback In this part, we present the experiments run with Relevance Feedback (Section 3.3) and applied using two different methods, Relevance Factor and Vocabulary Scoring. It should be noted that as there is no signi&#64257;cant difference between AIRP and MIRP, we excluded from the following experiments MIRP. We decided to use L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 101 Fig. 5. Results of AIRP IDF after the introduction of the Relevance Feedback using the Relevance Factor. We present, as well, the performance depending on the different positions from where we could obtain the r&#233;sum&#233;s for the Relevance Feedback. Table 4 Summary of the statistical analyses, done over the results, at 10 and 20 r&#233;sum&#233;s, re- garding the non-iterative Relevance Factor. The upper diagonal shows the p value of the results that were signi&#64257;cantly different. The lower diagonal shows the values of Cohen&#8217;s d effect size. 10 r&#233;sum&#233;s 20 r&#233;sum&#233;s Top Bottom Both Top Bottom Both Top 2.1 &#215; 10&#8722;10 1.7 &#215; 10&#8722;4 5.8 &#215; 10&#8722;9 5.0 &#215; 10&#8722;7 Bottom 0.532 2.3 &#215; 10&#8722;5 0.574 1.4 &#215; 10&#8722;3 Both 0.293 0.345 0.484 0.290 uniquely the AIRP IDF because it showed a statistical difference with AIRP, moreover, in real cases the IDF could be of help in re- ducing the n-grams that are frequent but useless for HRM. 6.2.1. Relevance Factor We present the results regarding the Relevance Factor and how the Relevance Feedback positions (Top, Bottom and Both) affected its performance. Furthermore, we veri&#64257;ed whether the iterative ap- plication of the Relevance Feedback could improve the speed of r&#233;- sum&#233; ranking. In each iterative step 2 r&#233;sum&#233;s were analyzed until reveal up to 20 r&#233;sum&#233;s. The results of these experiments are pre- sented in Fig. 5. In Fig. 5, we see that the Relevance Feedback depends on where the r&#233;sum&#233;s are obtained: Top, Bottom or Both positions. The Top position needs a smaller number of r&#233;sum&#233;s to generate higher values of MAP than the Bottom position does. The rANOVA done with 10 and 20 r&#233;sum&#233;s indicated a signif- icant difference between the positions in the non-iterative pro- cess, p value = 2.45 &#215; 10&#8722;12 and p value = 6.35 &#215; 10&#8722;11 respec- tively. More speci&#64257;cally, the pairwise post hoc test revealed that there was always a signi&#64257;cant difference with 10 and 20 r&#233;- sum&#233;s for all the Relevance Feedback positions (p value &lt; 0.005). In Table 4, we present a summary of the statistical analyses and the effect sizes obtained. It should be noted that the effect sizes are between medium-small and medium. Similar results for Relevance Feedback positions were obtained with the rANOVA and post hoc test for the iterative process. It can be seen, in Fig. 5, that the iterative application of the Rel- evance Feedback does not bring any improvement with respect to the non-iterative application. There are some minimal variations, positive or negative, but in most cases the values are the same. In fact, we determined through a rANOVA that there is no signi&#64257;- cant difference between the iterative and non-iterative application of the Relevance Feedback (p value &gt; 0.05) for 10 and 20 r&#233;sum&#233;s. We can say that both kinds of applications give comparable results. Thus, in the following experiments we use only the non-iterative process. 6.3. Relevance Factor with Vocabulary Scoring For the Relevance Factor with Vocabulary Scoring, we made use of AIRP IDF with a non-iterative Relevance Feedback application. Vocabulary Scoring was done following simulations S1, S2 and S3, as explained in Section 4.2. In Fig. 6, we present the results from these experiments. We see from Fig. 6 that the results in terms of the MAP depend on the simulation utilized for Vocabulary Scoring. On one hand, it is evident that simulation S3, where we used the maximum quan- tity of information available to calculate the Term Scores, com- pletely boosts the Relevance Factor and allows us reaching a MAP of 0.937 &#177; 0.014. On the other hand, simulations S1 and S2 do not improve the Relevance Factor. It can be seen in Fig. 6 that S1, de- spite being conceived to boost the n-grams that represented the classes, relevant and irrelevant, reduces the performance of the Relevance Factor in comparison to its application without Vocabu- lary Scoring. For instance, using 20 r&#233;sum&#233;s in the Relevance Feed- back process without any Vocabulary Scoring results in the MAP being equal to 0.800 &#177; 0.030, while using simulation S1 results in 102 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 Fig. 6. Results of AIRP IDF using a Relevance Feedback that was applied with the Relevance Factor and Vocabulary Scoring. The Vocabulary Scoring was obtained through three different simulation S1, S2 and S3. Table 5 Summary of the statistical analyses, done over the results, at 10 and 20 r&#233;sum&#233;s, regarding the application of Relevance Factor with three simulations of Vocabulary Scoring (S1, S2, S3). The upper diagonal shows the p value of the results that were signi&#64257;cantly different. The lower diagonal shows the values of Cohen&#8217;s d effect size. 10 r&#233;sum&#233;s 20 r&#233;sum&#233;s S1 S2 S3 S1 S2 S3 S1 1.2 &#215; 10&#8722;8 &lt; 2 &#215; 10&#8722;16 6.2 &#215; 10&#8722;14 &lt; 2 &#215; 10&#8722;16 S2 0.458 &lt; 2 &#215; 10&#8722;16 0.749 &lt; 2 &#215; 10&#8722;16 S3 0.978 0.858 1.163 1.013 a MAP value of 0.766 &#177; 0.031. In contrast, in S2, where we do not consider the representative n-grams of each class, the MAP stayed stable as if the Vocabulary Scoring would have not been used. This outcome, will be discussed in Section 7. The rANOVA performed on the results showed there was a sig- ni&#64257;cant difference between the simulations using 10 and 20 r&#233;- sum&#233;s, in both cases p value = 2.2 &#215; 10&#8722;16. According to the pair- wise post hoc test, at 10 and 20 r&#233;sum&#233;s, all the simulations were signi&#64257;cantly different. In Table 5, we present the results regarding p value and effect size. Regarding the effect size, at 10 r&#233;sum&#233;s, between simulation S1 and S2 Cohen&#8217;s d = 0.458, which is large-small; between S3 and, S1 and S2, Cohen&#8217;s d was greater than 0.850, which it is a large ef- fect size. Using 20 r&#233;sum&#233;s, the effect size between S1 and S2 was large-medium effect size d = 0.749, for the rest of pairs, Cohen&#8217;s d was greater than 1, which correspond to a large effect size. 2.2 times the number of irrelevant r&#233;sum&#233;s. This contrasts with the average number of relevant r&#233;sum&#233;s for the 171 job postings, which was 1.4 times the number of irrelevant r&#233;sum&#233;s. Another explanation, is that this difference can be a signal that the &#8220;true&#8221; MAP, the one that would be obtained if we analyze the statistical population instead of a statistical sample, is located between 0.60 and 0.73. Although these could be the main reasons, we do not leave aside the fact that there could be others, intrinsic or not, to these job postings. To &#64257;nd these other reasons, we need to per- form a deeper analysis of these job postings and validate whether the number of relevant r&#233;sum&#233;s had an impact on the performance of AIRP and MIRP. 7.2. Relevance Feedback positions and the Relevance Factor As we observed in Section 6.2.1, the Relevance Factor is affected by the place from where the r&#233;sum&#233;s used for the Relevance Feed- back were obtained. In fact, the most helpful position was the Top one while the Bottom position was the one that gave the lowest performance. The latter result indicates that at the end of the rank- ings we did not &#64257;nd relevant r&#233;sum&#233;s. In other words, we do not &#64257;nd r&#233;sum&#233;s that could help us determine what is sought by the HRM. As a consequence, it is di&#64259;cult to improve the results using only irrelevant r&#233;sum&#233;s. Moreover, in order to see an improvement L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 103 with the Bottom position, it is necessary to increase the number of analyzed r&#233;sum&#233;s. This means reaching the middle of the rankings, from the bottom, to increase the probability of &#64257;nding relevant r&#233;- sum&#233;s. Despite the Both position results were less performing than those obtained with the Top position, it could be of interest to fol- low it in real life. The main reason is that it may verify that we did not leave someone relevant at the end of the r&#233;sum&#233; ranking. The second reason is that its behavior is not far from the behavior ob- tained with the Top position; although according to the statistical test, there is a signi&#64257;cant difference and the effect size is between small and medium-small. It is of interest to determine whether an asymmetric Both po- sition is better than a symmetric one. Currently, the same number of r&#233;sum&#233;s is analyzed from the top and the bottom of the r&#233;sum&#233; rankings. However, it may be better to analyze more r&#233;sum&#233;s from the top of the rankings than from the bottom to improve the speed of our methods. It can be asked why the MAP decreases when using two r&#233;- sum&#233;s for Relevance Feedback for the Bottom and Both positions. The reason is that we increase the probability of &#64257;nding only ir- relevant r&#233;sum&#233;s by looking for r&#233;sum&#233;s at these positions. When we use only irrelevant r&#233;sum&#233;s for the Relevance Factor (Eq. (6)), we can penalize relevant r&#233;sum&#233;s based on their small similarities with the irrelevant ones. As mentioned previously, by increasing the number of analyzed r&#233;sum&#233;s, we can increase the number of relevant r&#233;sum&#233;s analyzed and reduce the effect of the irrelevant ones located at the end of the rankings. We did not found any signi&#64257;cant difference with respect to the iterative and non-iterative application of the Relevance Feed- back. Moreover, we do not have a precise idea of why the itera- tive application did not improve the speed of r&#233;sum&#233; ranking. The best idea that we have is that the improvement is so small that the MAP cannot detect it. Put differently, the r&#233;sum&#233;s just change ranking positions with other r&#233;sum&#233;s of the same type (relevant or irrelevant) and this cannot be detected by the MAP. It is pos- sible that the number of r&#233;sum&#233;s used in each iteration, two, is not enough to provide visible improvement. We may need to de- termine with other experiments how many r&#233;sum&#233;s are necessary in an iterative application of the Relevance Feedback to see real improvement. To improve the performance of the iterative application of the Relevance Feedback, we may need as well to take into account the history of how the r&#233;sum&#233;s move within the rankings. If we &#64257;nd that r&#233;sum&#233; rankings do not change greatly, it could mean that we arrived at a point where we cannot further improve the rankings with this method. Thus, we should change the method, for exam- ple, by using Vocabulary Scoring or looking for relevant r&#233;sum&#233;s at the bottom, or even at a random position. 7.3. Vocabulary Scoring The results obtained using Vocabulary Scoring and the Rele- vance Factor were surprising. We never expected to surpass a MAP of 0.9, as we did with S3 (MAP of 0.9372 &#177; 0.014). Furthermore, we were surprised by the results because Vocabulary Scoring only af- fects the model used in determining the Relevance Factor. Thus, the AIRP of one r&#233;sum&#233; r is modi&#64257;ed only by the Relevance Factor (Eq. (6)) which determines how proximal r&#233;sum&#233; r is to the rele- vant and irrelevant ones using basically 100 n-grams chosen by the HRM (50 terms per class). The poor performance of S1 and S2, seen in Section 6.3, may be related to the quantity of data utilized to establish the Term Scores. Using only the information provided by documents from the Relevance Feedback is not enough to simulate correctly the knowledge that an HRM would have about the job posting and, in consequence, to determine the Term Scores. It should be remem- bered that the simulations are based on the squared probabilities (Eq. (9)) and without enough information these values lack the re- liability to correctly represent the classes. Although, we tried to increase the reliability by using only n-grams observed in at least two r&#233;sum&#233;s, as explained in Section 4.2.1, this minimum might not be enough for these two simulations. The problem is solved when we make use of S3, where we calculate the squared proba- bilities based on all the information available. To better understand how the simulations worked and affected the results, we present in the following lines a discussion of the simulations generated regarding a Project Manager job posting; this job posting is one of the 60 job postings linked manually to the job offer. In Fig. 7, we present an abstract of the job offer related to the job posting. In Table 6, we present an extract of Vocabulary Scor- ing using the three simulations, S1, S2 and S3, for 20 r&#233;sum&#233;s of Relevance Feedback.23 It should be remembered, that for obtaining the n-grams and the values presented in Table 6, we did not make use of the job offer at any moment, they are result from simulation S1, S2 and S3 as explained in Section 4.2.1. We see from Table 6 that simulation S3 provides the best weights to the terms related to the job offer, even when the last one was not included in the analysis process. Nevertheless, S1 and S2 have trouble correctly weighting the terms of the job offer or at least placing them within the &#64257;rst &#64257;ve positions; the reason is the lack of information. Additionally, although impossible to show due to their length, it should be mentioned that for simulation S3, the n-grams of both classes always had a squared probability, p2 c (t), of 1. For simula- tions S1 and S2 the squared probabilities were always 1 regarding the relevant class, while they varied from 1 to 0.444 for the irrel- evant class. In general, thanks to outputs like those presented in Table 6, it is possible to better understand which characteristics were the ones looked for or impacted the decision of HRM. With this kind of lists, psychologist can do a posteriori studies regarding the selec- tion of candidates. Or, other HRMs can use this kind of output to explain to candidates why they were not selected for an interview. One interesting thing to note, as seen in Fig. 6, is that S2 is bet- ter than S1 despite the former did not contain the terms that were boosted in the latter. The reason for this discrepancy is related to the quality of the n-grams chosen for the simulations and how we determine the Term Scores. As seen in Table 6, the terms used for simulations S1 and S2, especially those for the relevant r&#233;sum&#233;s, are quite different from the terms found in S3 and in the job offer. They can be considered as &#8220;bad&#8221; in terms of representativeness. Thus, in S1 we gave these &#8220;bad&#8221; n-grams the power to re&#64258;ect the classes, even though they do not truly represent them; the con- sequences are bad rankings. In S2 we deleted these &#8220;bad&#8221; terms, while the rest of terms represented the classes, although with poor Term Scores; the resulting rankings are affected negatively but not as much as in S1. In the previous results, we can see that the terms chosen by HRM may have a crucial role in the performance of Vocabulary Scoring, and as a consequence on the performance of the Rele- vance Factor. In other words, to choose terms that do not correctly represent what an HRM wants and does not want can negatively impact the ranking of r&#233;sum&#233;s. Related to this last point, we want to know how the Vocabulary Scoring is affected by the way the terms are sorted because it may not be an obvious task for an HRM to perform. In fact, an HRM 23 Simulations S1 and S2 sort in the same way the n-grams; their difference is that S2 gives a Term Score of 0 to the &#64257;rst 50 n-grams. Simulation S3 makes use of all the information available in the job to sort the terms presented in the 20 r&#233;sum&#233;s analyzed. 104 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 Fig. 7. Summary of a Project Manager job offer. The job offer comes from one of the 60 job postings to which we found their respective job offers. The original job offer was in French; we translated it to English and summarized it. Table 6 Squared probabilities, sum of weights, number of documents, factors and rank for a set of terms ac- cording to each Vocabulary Scoring simulation. All the n-grams, originally in French but translated to English, belong to the r&#233;sum&#233;s linked to the job offer presented in Fig. 7. The job has in total 36 rele- vant r&#233;sum&#233;s and 29 irrelevant ones. Simulations Class n-gram (t) pc(t) \x05W(t) Dc(t) fc(t) Rank S1 and S2 Irrelevant Project engineer 1 0.024 3 0.072 1 Micro-techniques 1 0.022 2 0.045 2 Investment 1 0.013 3 0.040 3 SolidWorks Catia V5 1 0.019 2 0.039 4 Supplier France 1 0.019 2 0.039 5 Relevant Business 1 0.040 7 0.285 1 Rail 1 0.030 7 0.216 2 Planning 1 0.024 8 0.196 3 Range 1 0.023 8 0.189 4 Respect 1 0.024 7 0.174 5 S3 Irrelevant Responsible supplier 1 0.038 4 0.154 1 Unit 1 0.026 4 0.106 2 Renault project 1 0.032 3 0.098 3 To orient 1 0.024 4 0.096 4 Validation piece 1 0.041 2 0.083 5 Relevant Rail 1 0.023 22 5.098 1 Alstom transport 1 0.074 8 0.598 2 Train 1 0.076 7 0.532 3 TGV 1 0.062 6 0.372 4 CAD software 1 0.048 5 0.241 5 can ask how to determine whether one term better represents the relevant or irrelevant r&#233;sum&#233;s than another one. Moreover, they can question whether to &#8220;incorrectly&#8221; sort one term would affect the resulting ranking at the same level as choosing a bad term. To answer these questions, instead of computing the Term Score with Eq. (7), we decided to assign a Term Score of 1 to the 50 more representative n-grams of each class. This is equivalent to saying that the order in which the n-grams are sorted has no importance. The results of setting the Term Scores equal to 1 using simula- tion S3 showed that at 10 r&#233;sum&#233;s, we get a MAP of 0.913 &#177; 0.015; at 20 r&#233;sum&#233;s, the MAP is 0.947 &#177; 0.012. The rANOVA between our method using Term Scores set to 1 and those computed with the 5th root indicated there is no signi&#64257;cant difference at 10 and 20 r&#233;sum&#233;s (p value = 1.7 &#215; 10&#8722;3 and p value = 1.37 &#215; 10&#8722;9 respec- tively). These outcomes do not mean that both methods are equiv- alent and as a consequence interchangeable, but that they perform very similarly.24 As well, the results obtained from using a Term Score of 1 may provide a hint that the success of Vocabulary Scor- ing is related more to the quality of the chosen n-grams and the weight difference we create with respect to the other terms, i.e., those to which we set a Term Score of 0.01. In other words, to put the most representative n-gram at the 50th position of the Vocabu- lary Scoring does not affect the results as much as leaving it aside. One interesting thing we observed in &#64257;ve different job post- ings using S3 is that the top ranked n-grams from the relevant r&#233;sum&#233;s appear in more documents than the top ranked n-grams from the irrelevant r&#233;sum&#233;s. We see this behavior in column Dc(t) 24 The lack of signi&#64257;cant difference between two means does not express that they are equal. It indicates that we need more data to determine a signi&#64257;cant difference. However, the effect size of this difference may be very small and, in consequence, they would behave very similar in real conditions. of Table 6. If this is true for all the job postings, we could con&#64257;rm the ideas on which we based AIRP and MIRP: the r&#233;sum&#233;s from relevant applicants have in common multiple terms while the ir- relevant r&#233;sum&#233;s usually present a great variety of terms that are not frequently shared. However, we must perform a deeper analy- sis to validate this hypothesis. Despite the interest to determine what would be the results us- ing human judgments instead of simulations, it should be noted that this cannot be done without redoing the selection process. The main reason is the relation between the selection of appli- cants and the person speci&#64257;cation, a document that can evolve over time. In other words, the HRM who would redo the selec- tion process may not have access to the previous person speci- &#64257;cation. This may result in a different evaluation of r&#233;sum&#233;s, es- pecially those from the &#64257;rst candidates who applied. However, we can imagine that in reality, humans would do a good job, even bet- ter than simulations, because they know a priori the person speci- &#64257;cation. Although we did not test Vocabulary Scoring with a set of less than 50 n-grams, it may be possible to reduce this &#64257;gure. In &#64257;rst place, we should test whether a smaller Vocabulary Scoring with Term Scores set to 1, or determined by Eq. (7), have the same per- formance. If this is not the case, we may change Eq. (7). For ex- ample, a gradient closer to zero might help to give better results to the top 10 terms. Another option would be to further reduce the Term Score for the n-grams that do not appear in the Rele- vance Feedback. In previous experiments, not presented here, we observed that as we decreased the Term Scores of the unseen n- grams the results were boosted even more. Moreover, it could be of help to &#64257;nd the n-grams or terms, and even their synonyms, that appear in the job offer and per- son speci&#64257;cation in order to improve or automate the generation L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 105 of Vocabulary Scorings. In other words, these n-grams or terms could be those that should be positioned at the top of the Vo- cabulary Scoring. To this end, we could make use of Human Re- sources lexica, ontologies and terminological extractors. However, the use of these resources may introduce some di&#64259;culties as terms may not correspond exactly to the n-grams used in the vector model. 
</corps><discussion>
7. Discussion In the following subsections, we discuss the results obtained in Section 6. The discussion is divided based on the experiments. 7.1. AIRP, MIRP, IDF and baselines The signi&#64257;cant difference between our methods and the ran- dom baseline method means that our methods can be, by them- selves, of help to HRMs. In other words, the Inter-R&#233;sum&#233; Proxim- ity, used through AIRP and MIRP, can rank correctly, to a certain degree, the r&#233;sum&#233;s and proposes a better start point, than a ran- dom one, to HRMs during the selection process. As we observed in Section 6.1, there was no signi&#64257;cant difference between AIRP and MIRP. This &#64257;nding means that the distribution of Inter-R&#233;sum&#233; Proximities is often symmetrical and does not contain outliers. We observed that between all our methods and the random baseline there was a statistical difference, however between our other methods, in general, there was not a signi&#64257;cant difference. Moreover, the rANOVA performed on the results presented over the subset of 60 job postings (Fig. 4) suggests that our methods are better than the method based on the similarity between job offers and r&#233;sum&#233;s. We could see this, as evidence that r&#233;sum&#233;s contain more information about the job requirements than the job offer does, at least without using semantic resources. This could also mean that the vocabulary used in the job offer and the r&#233;- sum&#233;s differs to a certain degree. It is interesting how in terms of MAP, our methods worked better over the 60 job postings to which we had access to the job offer than for the set of 171 job postings. One reason for this outcome might be that these 60 job postings had one particular characteristic: on average, the number of relevant r&#233;sum&#233;s was 

</discussion><conclusion>8. Conclusions and future work The massive access of the Internet has changed multiple as- pects of our lives, and the way we &#64257;nd and apply for a job offer is not an exception. Although the use of computers and the In- ternet has made easier to &#64257;nd job offers and potential candidates to send their r&#233;sum&#233;s or curricula vitae, it has negatively affected the performance of human resource managers during the selection process. Human resource managers have trouble to &#64257;nd rapidly the candidates, among all who applied, that meet the job requirements and should be called for an interview. We presented two innovative methods for ranking r&#233;sum&#233;s by relevance, making it easier for human resource managers to iden- tify candidates with the desired characteristics. The methods here presented are innovative because they make use only of the r&#233;- sum&#233;s sent in response to a job offer. These methods contrast with state-of-the-art methods that usually compare r&#233;sum&#233;s and job offers with proximity measures. Our methods are language in- dependent and do not need semantic resources to work. Moreover, the methods presented here are statistically better than a random baseline or a baseline grounded on the similarity between r&#233;sum&#233;s and a job offer. Moreover, we presented two different ways to apply Relevance Feedback in a r&#233;sum&#233; ranker. One method for applying Relevance Feedback works at a general level (Relevance Factor), while the other method works at a &#64257;ner lexical one (Vocabulary Scoring). Al- though the Relevance Factor helps to improve r&#233;sum&#233; rankings, we &#64257;nd that it is its use along with Vocabulary Scoring that helps us to reach a Mean Average Precision of 0.937. Put differently, by us- ing the Relevance Factor with Vocabulary Scoring we can correctly rank almost every r&#233;sum&#233;. As a consequence, we can reduce the time needed by human resource managers to &#64257;nd the r&#233;sum&#233;s of relevant applicants. It is important to note that the very good re- sults obtained with Vocabulary Scoring reinforces the concept that relevant r&#233;sum&#233;s share more characteristics with themselves than with irrelevant ones, as seen in our previous works. We believe that, within the r&#233;sum&#233;s we can intrinsically &#64257;nd a &#8220;facial composite&#8221; of the ideal candidate, and possibly the &#8220;facial composite&#8221; that represents the unquali&#64257;ed candidates. It may be these &#8220;facial composites&#8221; that enable us to rank r&#233;sum&#233;s without the use of a job offer or semantic resources. We consider that methodologies based only on r&#233;sum&#233;s and their vocabularies are the future of r&#233;sum&#233; rankers. The main rea- son to think this is that they are capable of offering excellent performance without being limited to one domain or language. Despite these methods were created to be used in a particular database, where it was impossible to have access to every job of- fer, we believe that it can be used in any database of r&#233;sum&#233;s, only if these are separated by job postings. Furthermore, the methods here presented do not make use of any kind of semantic resources, which can make them easier to implement in under-resources lan- guages. There are still things that must be studied with this kind of methods. In the &#64257;rst place are the temporal aspects. We assumed in this article that all the r&#233;sum&#233;s were present at the same time, but in real life this may not be true. On occasions, the process of recruitment and selection are done in parallel, i.e., once a r&#233;- sum&#233; arrives to a human resource manager, it is analyzed. We have to consider as well the evolution of the person speci&#64257;cation over time. In some cases, human resource managers are obliged to be- come more or less strict in order to &#64257;lter the applicants. These changes, in consequence, will affect the human resource managers&#8217; perception regarding the relevance of applicants. Due to this effect, the way to apply our methods may need to change, and we should evaluate until which extent they remain valid. However, despite all, the proposed methods could be used to evaluate a posteriori the reasons why a group of candidates was chosen to do an interview. Moreover, other human resource managers or psychologists may &#64257;nd useful the tool to determine whether human resource man- agers were affected by personality inferences, misspellings or any kind of discrimination. Another aspect to take into account is the way to match terms or concepts and n-grams. These representations are not the same, and this can infuse di&#64259;culty to some degree in the application of our methods. Put differently, a concept may be di&#64259;cult to repre- sent with an n-gram. Finally, it should be analyzed the economics and whether human resource managers will adopt these methods to make their tasks easier. Regarding the scalability of the methods here presented, we do not observe any particular problem. As we indicated in Section 5, the methods were called using the program GNU Parallel, meaning that each job posting was analyzed using different CPU threads. This indicates that multiple job postings can be processed at the same time without any collision. Furthermore, it is possible to par- allelize the similarity between r&#233;sum&#233;s, i.e., to use several threads to calculate multiple Dice&#8217;s Coe&#64259;cient scores at the same time. The only aspect to take into consideration is that the vectors represent- ing the r&#233;sum&#233;s should be accessible to every thread. At the end, all the methods described in this work can be easily scaled and distributed in a cluster. In the future, we would like to use word embedding in order to calculate the proximity between r&#233;sum&#233;s differently. It could also be useful for Vocabulary Scorings. In addition, we will work on the improvements described in the discussion. Since the meth- ods developed here are language independent, it will be easy to test them on other languages than French. Although this last task can be di&#64259;cult to achieve due to the lack of a corpus of real se- lection processes. During the experimentation, we observed that our methods can keep a good performance when they are tested on an encrypted version of the data set here used.25 Therefore, we can rely on this clue that for other languages, the methods should work as well. In conclusion, we hope that our methods and results will attract new and deeper research in this domain. Credit authorship contribution statement Luis Adri&#225;n Cabrera-Diego: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing - original draft, Writing - review &amp; editing, Visualiza- tion. Marc El-B&#233;ze: Conceptualization, Methodology, Validation, Formal analysis, Investigation, Writing - review &amp; editing, Super- vision, Project administration, Funding acquisition. Juan-Manuel Torres-Moreno: Conceptualization, Methodology, Writing - review &amp; editing, Supervision, Project administration, Funding acquisi- tion. Barth&#233;l&#233;my Durette: Conceptualization, Methodology, Formal analysis, Writing - review &amp; editing, Supervision, Project adminis- tration. 25 We did not achieve the same results in the encrypted data set, as the r&#233;sum&#233;s were encrypted without doing a deep pre-processing, like lemmatization or stop words deletion. Thus, the r&#233;sum&#233;s contained a greater variety of terms and noisy words. 106 L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 Acknowledgments This work was partially funded by the Agence National de la Recherche et de la Technologie (ANRT), France, through the CIFRE convention 2012/0293b and by the Consejo Nacional de Ciencia y Tecnolog&#237;a (CONACyT), Mexico, with the grant 327165. 

</conclusion><biblio>References Armstrong, M., &amp; Taylor, S. (2014). Armstrong&#8217;s handbook of human resource manage- ment practice (13th). Kogan Page Publishers. Arthur, D. (2001). The employee recruitment and retention handbook. AMACOM. Barber, L. (2006). E-Recruitment developments. Institute for Employment Studies. Buckley, C., &amp; Voorhees, E. M. (2000). Evaluating evaluation measure stability. In Proceedings of the 23rd annual international ACM SIGIR conference on research and development in information retrieval (pp. 33&#8211;40). Athens, Greece: ACM. doi:10. 1145/345508.345543. Cabrera-Diego, L. A. (2015). Automatic methods for assisted recruitment. Universit&#233; d&#8217;Avignon et des Pays de Vaucluse Ph.D. thesis. Cabrera-Diego, L. A., Durette, B., Lafon, M., Torres-Moreno, J.-M., &amp; El-B&#232;ze, M. (2015). How can we measure the similarity between r&#233;sum&#233;s of selected candidates for a job?. In Stahlbock, Robert, &amp; Weiss, Gary M. (Eds.), Proceedings of the 11th international conference on data mining (DMIN&#8217;15) (pp. 99&#8211;106). Las Vegas, USA Chapman, D. S., &amp; Webster, J. (2003). The use of technologies in the recruiting, screening, and selection processes for job candidates. International Journal of Se- lection and Assessment, 11(2&#8211;3), 113&#8211;120. doi:10.1111/1468-2389.00234. Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd). Hillsdale, USA: Lawrence Earlbaum Associates. Cole, M. S., Feild, H. S., Giles, W. F., &amp; Harris, S. G. (2009). Recruiters&#8217; infer- ences of applicant personality based on r&#233;sum&#233; screening: Do paper people have a personality? Journal of Business and Psychology, 24(1), 5&#8211;18. doi:10.1007/ s10869-008-9086-9. Cossu, J.-V. (2015). Analyse de l&#8217;image de marque sur le Web 2.0. Avignon, France: Universit&#233; d&#8217;Avignon et des Pays de Vaucluse Ph.D. thesis. Cossu, J.-V., Janod, K., Ferreira, E., Gaillard, J., &amp; El-B&#232;ze, M. (2014). LIA@RepLab 2014: 10 methods for 3 tasks. In L. Cappellato, N. Ferro, M. Halvey, &amp; W. Kraaij (Eds.), Working notes for 4th International Conference of the CLEF initiative (pp. 1458&#8211;1467). She&#64259;eld, UK Elkington, T. (2005). Bright future for online recruitment. Personnel Today, 9. Faliagka, E., Iliadis, L., Karydis, I., Rigou, M., Sioutas, S., Tsakalidis, A., &amp; Tz- imas, G. (2013). On-line consistent ranking on e-recruitment: Seeking the truth behind a well-formed CV. Arti&#64257;cial Intelligence Review, 1&#8211;14. doi:10.1007/ s10462-013-9414-y. Faliagka, E., Kozanidis, L., Stamou, S., Tsakalidis, A., &amp; Tzimas, G. (2011). A person- ality mining system for automated applicant ranking in online recruitment sys- tems. In S. Auer, O. D&#237;az, &amp; G. A. Papadopoulos (Eds.), Proceedings of the 11th international conference web engineering (ICWE 2011). In Lecture Notes in Com- puter Science: 6757 (pp. 379&#8211;382). Paphos, Cyprus: Springer Berlin Heidelberg. doi:10.1007/978-3-642-22233-7_30. Fang, X., &amp; Zhan, J. (2015). Sentiment analysis using product review data. Journal of Big Data, 2(1), 5. doi:10.1186/s40537-015-0015-2. Garc&#237;a-S&#225;nchez, F., Mart&#237;nez-B&#233;jar, R., Contreras, L., Fern&#225;ndez-Breis, J. T., &amp; Castellanos-Nieves, D. (2006). An ontology-based intelligent system for recruit- ment. Expert Systems with Applications, 31(2), 248&#8211;263. doi:10.1016/j.eswa.2005. 09.023. Guo, S., Alamudun, F., &amp; Hammond, T. (2016). R&#233;suMatcher: A personalized r&#233;sum&#233;- job matching system. Expert Systems with Applications, 60(Supplement C), 169&#8211; 182. doi:10.1016/j.eswa.2016.04.013. Harzallah, M., Lecl&#232;re, M., &amp; Trichet, F. (2002). CommOnCV: Modelling the compe- tencies underlying a curriculum vitae. In Proceedings of the 14th international conference on software engineering and knowledge engineering (SEKE&#8217;02) (pp. 65&#8211; 71). Ischia Island, Italy: ACM. doi:10.1145/568760.568773. Hutterer, M. (2011). Enhancing a job recommender with implicit user feedback. Vienna, Austria: Fakult&#228;t f&#252;r Informatik der Technischen Universit&#228;t Wien Master&#8217;s thesis. J&#228;rvelin, K., &amp; Kek&#228;l&#228;inen, J. (2000). IR evaluation methods for retrieving highly rel- evant documents. In Proceedings of the 23rd annual international ACM SIGIR con- ference on research and development in information retrieval (pp. 41&#8211;48). Athens, Greece: ACM. doi:10.1145/345508.345545. Kessler, R., B&#233;chet, N., Roche, M., El-B&#232;ze, M., &amp; Torres-Moreno, J. M. (2008a). Au- tomatic pro&#64257;ling system for ranking candidates answers in human resources. In R. Meersman, Z. Tari, &amp; P. Herrero (Eds.), On the move to meaningful inter- net systems: OTM 2008 Workshops. In Lecture Notes in Computer Science: 5333 (pp. 625&#8211;634). Monterrey, Mexico: Springer Berlin Heidelberg. doi:10.1007/ 978-3-540-88875-8_86. Kessler, R., B&#233;chet, N., Roche, M., Torres-Moreno, J.-M., &amp; El-B&#232;ze, M. (2012). A hy- brid approach to managing job offers and candidates. Information Processing &amp; Management, 48(6), 1124&#8211;1135. doi:10.1016/j.ipm.2012.03.002. Kessler, R., B&#233;chet, N., Torres-Moreno, J.-M., Roche, M., &amp; El-B&#232;ze, M. (2009). Job offer management: How improve the ranking of candidates. In Foundations of intelligent systems: Proceedings of 18th international symposium on methodologies for intelligent systems (ISMIS 2009). In Lecture Notes in Computer Science: 5722 (pp. 431&#8211;441). Prague, Czech Republic: Springer Berlin Heidelberg. doi:10.1007/ 978-3-642-04125-9_46. Kessler, R., Torres-Moreno, J. M., &amp; El-B&#232;ze, M. (2008b). E-Gen: Pro&#64257;lage automa- tique de candidatures. In Actes de la 15&#232;me conf&#233;rence sur le Traitement Automa- tique des Langues Naturelles (TALN 2008) (pp. 370&#8211;379). Avignon, France Kmail, A. B., Maree, M., &amp; Belkhatir, M. (2015). MatchingSem: Online recruitment system based on multiple semantic resources. In 12th international conference on fuzzy systems and knowledge discovery (FSKD 2015) (pp. 2654&#8211;2659). doi:10. 1109/FSKD.2015.7382376. Looser, D., Ma, H., &amp; Schewe, K.-D. (2013). Using formal concept analysis for ontol- ogy maintenance in human resource recruitment. In F. Ferrarotti, &amp; G. Gross- mann (Eds.), Proceedings of the ninth Asia-Paci&#64257;c conference on conceptual mod- elling: 143 (pp. 61&#8211;68). Adelaide, Australia: Australian Computer Society, Inc. Martin-Lacroux, C. (2017). &#8220;Without the spelling errors I would have shortlisted her...&#8221;:The impact of spelling errors on recruiters&#8217; choice during the personnel selection process. International Journal of Selection and Assessment, 25(3), 276&#8211; 283. doi:10.1111/ijsa.12179. Martinez-Gil, J., Paoletti, A. L., R&#225;cz, G., Sali, A., &amp; Schewe, K.-D. (2018). Accurate and e&#64259;cient pro&#64257;le matching in knowledge bases. Data &amp; Knowledge Engineering, 117, 195&#8211;215. doi:10.1016/j.datak.2018.07.010. Martinez-Gil, J., Paoletti, A. L., &amp; Schewe, K.-D. (2016). A smart approach for match- ing, learning and querying information from the human resources domain. In M. Ivanovic&#769;, B. Thalheim, B. Catania, K.-D. Schewe, M. Kirikova, P. &#352;aloun, A. Da- hanayake, T. Cerquitelli, E. Baralis, &amp; P. Michiardi (Eds.), Proceedings of the new trends in databases and information systems: ADBIS 2016 short papers and work- shops, BigDap, DCSA, DC (pp. 157&#8211;167). Prague, Czech Republic: Springer Inter- national Publishing. doi:10.1007/978-3-319-44066-8_17. Mason, R. L., Gunst, R. F., &amp; Hess, J. L. (2003). Statistical design and analysis of exper- iments: With applications to engineering and science. Wiley Series in Probability and Statistics (2nd). Wiley-Interscience. doi:10.1002/0471458503. Menon, V. M., &amp; Rahulnath, H. A. (2016). A novel approach to evaluate and rank can- didates in a recruitment process by estimating emotional intelligence through social media data. In International conference on next generation intelligent sys- tems (ICNGIS) (pp. 1&#8211;6). Kottayam, India: IEEE. doi:10.1109/ICNGIS.2016.7854061. Montuschi, P., Gatteschi, V., Lamberti, F., Sanna, A., &amp; Demartini, C. (2014). Job re- cruitment and job seeking processes: How technology can help. IT Professional, 16(5), 41&#8211;49. doi:10.1109/MITP.2013.62. Padr&#243;, L., &amp; Stanilovsky, E. (2012). FreeLing 3. 0: Towards wider multilinguality. In N. Calzolari, K. Choukri, T. Declerck, M. U. Dog&#774;an, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, &amp; S. Piperidis (Eds.), Proceedings of the eight international conference on language resources and evaluation (LREC&#8217;12) (pp. 2473&#8211;2479). Is- tanbul, Turkey: ELRA. R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing Vienna, Austria. Radevski, V., &amp; Trichet, F. (2006). Ontology-based systems dedicated to human re- sources management: An application in e-Recruitment. In R. Meersman, Z. Tari, &amp; P. Herrero (Eds.), On the move to meaningful internet systems 2006: OTM 2006 Workshops. In Lecture Notes in Computer Science: 4278 (pp. 1068&#8211;1077). Montpellier, France: Springer Berlin Heidelberg. doi:10.1007/11915072_9. Rocchio, J. J. (1971). Relevance feedback in information retrieval. In G. Salton (Ed.), The SMART retrieval system: Experiments in automatic document processing. In Au- tomatic Computation (pp. 313&#8211;323). Englewood Cliffs, N.J., USA: Prentice-Hall. Salton, G., Wong, A., &amp; Yang, C.-S. (1975). A vector space model for automatic index- ing. Communications of the ACM, 18(11), 613&#8211;620. doi:10.1145/361219.361220. Sen, A., Das, A., Ghosh, K., &amp; Ghosh, S. (2012). Screener: A system for extracting ed- ucation related information from resumes using text based information extrac- tion system. In Proceedings of 2012 international on computer and software model- ing (ICCSM 2012). In International proceedings of computer science &amp; information technology: 54 (pp. 31&#8211;35). International Association of Computer Science and Information Technology Press (IACSIT Press). doi:10.7763/IPCSIT.2012.V54.06. Senthil Kumaran, V., &amp; Sankar, A. (2012). Expert locator using concept linking. Inter- national Journal of Computational Systems Engineering, 1(1), 42&#8211;49. doi:10.1504/ IJCSYSE.2012.044742. Senthil Kumaran, V., &amp; Sankar, A. (2013). Towards an automated system for in- telligent screening of candidates for recruitment using ontology mapping (EX- PERT). International Journal of Metadata, Semantics and Ontologies, 8(1), 56&#8211;64. doi:10.1504/IJMSO.2013.054184. Singh, A., Rose, C., Visweswariah, K., Chenthamarakshan, V., &amp; Kambhatla, N. (2010). PROSPECT: A system for screening candidates for recruitment. In Proceedings of the 19th ACM international conference on information and knowledge manage- ment (CIKM 2010) (pp. 659&#8211;668). Toronto, Canada: ACM. doi:10.1145/1871437. 1871523. Sp&#228;rck-Jones, K. (1972). A statistical interpretation of term speci&#64257;city and its appli- cation in retrieval. Journal of Documentation, 28(1), 11&#8211;21. doi:10.1108/eb026526. Tange, O. (2011). GNU parallel - The command-line power tool. login: The USENIX Magazine, 36(1), 42&#8211;47. Thompson, M. A. (2000). The global resume and CV guide. Chichester, New York: Wi- ley. Tinelli, E., Colucci, S., Donini, F. M., Di Sciascio, E., &amp; Giannini, S. (2017). Embedding semantics in human resources management automation via SQL. Applied Intelli- gence, 46(4), 952&#8211;982. doi:10.1007/s10489-016-0868-x. Torres-Moreno, J.-M., El-B&#232;ze, M., Bellot, P., &amp; B&#233;chet, F. (2012). Opinion detection as a topic classi&#64257;cation problem. In &#201;. Gaussier, &amp; F. Yvon (Eds.), Textual information access: Statistical models (pp. 337&#8211;368). Wiley-ISTE. doi:10.1002/9781118562796. ch9. L.A. Cabrera-Diego, M. El-B&#233;ze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019) 91&#8211;107 107 Trichet, F., Bourse, M., Lecl&#232;re, M., &amp; Morin, E. (2004). Human resource management and semantic web technologies. In Proceedings of information and communica- tion technologies: From theory to applications (ICTTA&#8217;04) (pp. 641&#8211;642). Damas- cus, Syria: IEEE. doi:10.1109/ICTTA.2004.1307928. Voorhees, E. M., &amp; Harman, D. (2001). Overview of TREC 2001. In Proceedings of the 10th Text REtrieval Conference (TREC 2001) (pp. 1&#8211;15). Gaithersburg, Maryland, USA: National Institute of Standards and Technology (NIST). Zaroor, A., Maree, M., &amp; Sabha, M. (2017). A hybrid approach to conceptual classi&#64257;cation and ranking of resumes and their corresponding job posts. In I. Czarnowski, R. J. Howlett, &amp; L. C. Jain (Eds.), Intelligent decision technologies 2017: Proceedings of the 9th KES international conference on intelligent decision technologies (KES-IDT 2017) - part I (pp. 107&#8211;119). Vilamoura, Portugal: Springer International Publishing. doi:10.1007/978-3-319-59421-7_10. 
</biblio>
</article>
</opml>'



<opml version="1.0">
	<article>
		<preamble>PERFECT
		</preamble>PERFECT
		
		<title>PERFECT
		</title>PERFECT
		
		<auteur>OK -1
		</auteur>PERFECT
		
		<abstract>NON DETECT
		</abstract>NON DETECT
		
		<introduction>PERFECT
		</introduction>INCORRECT
		
		<corps>PERFECT
		</corps>PERFECT
		
		<discussion>PERFECT
		</discussion>INCORRECT
		
		<conclusion>PERFECT
		</conclusion>PERFECT
		
		<biblio>PERFECT
		</biblio>PERFECT
	</article>
</opml>


Precision souple : 14/18
Precision stricte : 13/18